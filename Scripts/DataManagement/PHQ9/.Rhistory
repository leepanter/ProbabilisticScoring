coef.CD19Mala=c(coef.loglmod,coef.logLMwFEint,coef.logLMMwREint,
coef.logLMMwREslope,coef.logGEE)
(perctChange.coefCD19Mala=PerctChangeMatrix(coef.CD19Mala))
# Create Standard Error Vector
se.loglmod        =1.455*10 **(-2)
se.logLMwFEint    =1.381*10 **(-2)
se.logLMMwREint   =1.374*10  **(-2)
se.logLMMwREslope =3.538*10 **(-2)
se.logGEE         =1.455*10 **(-2)
# Calculate SE Percent Change matrix
se.CD19Mala=c(se.loglmod,
se.logLMwFEint,
se.logLMMwREint,
se.logLMMwREslope,
se.logGEE)
(perctChange.seCD19Mala=PerctChangeMatrix(se.CD19Mala))
# Create Test Statistic Vector
testStat.loglmod        = 3.381
testStat.logLMwFEint    = 3.5
testStat.logLMMwREint   = 3.579
testStat.logLMMwREslope = 1.678
testStat.logGEE         = 3.381
testStat.CD19Mala=c(testStat.loglmod,
testStat.logLMwFEint,
testStat.logLMMwREint,
testStat.logLMMwREslope,
testStat.logGEE)
(perctChange.testStatCD19Mala=PerctChangeMatrix(testStat.CD19Mala))
# ResultStatsCD34Fbln.R
# Nested Model Comparisons and Percent Change Matrices
# Load Data
load("/Users/lee/Desktop/CD34FblnWS.RData")
# Packages
library(nlme)
library(stargazer)
# We first Calculate the nested model comparisons
loglmod0.nlme=gls(model = logfbln~logcd34,
data = dat)
logLMMwREint.nlme=lme(fixed=logfbln~1 + logcd34,
random=~1|subject.no,
data = dat)
logLMMwREslope.nlme=lme(fixed=logfbln~1 + logcd34,
random=~(1+logfbln|subject.no),
data = dat)
## Model 0 < Model 1
anova(loglmod0, logLMwFEint)
## Model 0 < Model 3
x=anova.lme(loglmod0.nlme,logLMMwREint.nlme)
## Model 0 < Model 3
anova.lme(loglmod0.nlme,logLMMwREslope.nlme)
# Define Percent Change Function
PerctChange=function(bef,aft){
out=round(((aft-bef)/bef)*100, digits = 4)
return(out)
}
# Function to calculate Percent Change Matrices
PerctChangeMatrix=function(vec){
len=length(vec)
out.mat=matrix(NA, nrow = len, ncol = len)
for(i in 1:len){
for(j in 1:len){
out.mat[i,j]=PerctChange(vec[i], vec[j])
}
}
return(out.mat)
}
summary(logGEEmod)
View(loglmod0s)
loglmod0s
View(logLMwFEint)
summary(logLMwFEint)
# ResultStatsCD34Fbln.R
# Nested Model Comparisons and Percent Change Matrices
# Load Data
load("/Users/lee/Desktop/CD34FblnWS.RData")
# Packages
library(nlme)
library(stargazer)
# We first Calculate the nested model comparisons
loglmod0.nlme=gls(model = logfbln~logcd34,
data = dat)
logLMMwREint.nlme=lme(fixed=logfbln~1 + logcd34,
random=~1|subject.no,
data = dat)
logLMMwREslope.nlme=lme(fixed=logfbln~1 + logcd34,
random=~(1+logfbln|subject.no),
data = dat)
## Model 0 < Model 1
anova(loglmod0, logLMwFEint)
## Model 0 < Model 3
x=anova.lme(loglmod0.nlme,logLMMwREint.nlme)
## Model 0 < Model 3
anova.lme(loglmod0.nlme,logLMMwREslope.nlme)
# Define Percent Change Function
PerctChange=function(bef,aft){
out=round(((aft-bef)/bef)*100, digits = 4)
return(out)
}
# Function to calculate Percent Change Matrices
PerctChangeMatrix=function(vec){
len=length(vec)
out.mat=matrix(NA, nrow = len, ncol = len)
for(i in 1:len){
for(j in 1:len){
out.mat[i,j]=PerctChange(vec[i], vec[j])
}
}
return(out.mat)
}
# Create Coefficient Vector:
coef.loglmod=7.884*10**(-1)
coef.logLMwFEint=1.31*10**(-1)
coef.logLMMwREint=1.35*10**(-1)
coef.logLMMwREslope=1.705*10**(-1)
coef.logGEE=7.884*10**(-1)
# Calculate Coeficient percentage of change
coef.CD34Fbln=c(coef.loglmod,coef.logLMwFEint,coef.logLMMwREint,
coef.logLMMwREslope,coef.logGEE)
(perctChange.coef.CD34Fbln=PerctChangeMatrix(coef.CD34Fbln))
summary(logLMMwREslope)
summary(logGee)
summary(logGEEmod)
# Create Standard Error Vector
se.loglmod        =4.92*10**(-2)
se.logLMwFEint    =3.42*10**(-2)
se.logLMMwREint   =3.42*10**(-2)
se.logLMMwREslope =7.29*10**(-2)
se.logGEE         =4.92*10**(-2)
# Calculate SE Percent Change matrix
se.CD34Fbln=c(se.loglmod,
se.logLMwFEint,
se.logLMMwREint,
se.logLMMwREslope,
se.logGEE)
(perctChange.seCD34Fbln=PerctChangeMatrix(se.CD34Fbln))
# Calculate SE Percent Change matrix
testStat.CD34Fbln=c(testStat.loglmod,
testStat.logLMwFEint,
testStat.logLMMwREint,
testStat.logLMMwREslope,
testStat.logGEE)
(perctChange.testStatCD34Fbln=PerctChangeMatrix(testStat.CD34Fbln))
# Create Test Statistic Vector
testStat.loglmod        = 4.002
testStat.logLMwFEint    = 3.818
testStat.logLMMwREint   = 3.95
testStat.logLMMwREslope = 2.34
testStat.logGEE         = 4.002
# Calculate SE Percent Change matrix
testStat.CD34Fbln=c(testStat.loglmod,
testStat.logLMwFEint,
testStat.logLMMwREint,
testStat.logLMMwREslope,
testStat.logGEE)
(perctChange.testStatCD34Fbln=PerctChangeMatrix(testStat.CD34Fbln))
# ResultStatsCD19Mala.R
# Nested Model Comparisons and Percent Change Matrices
# Load Data
load("/Users/lee/Google Drive/RBC Project/Data and Scripts/FinalData/CompiledDataEnvironments/CD19MalaWS.RData")
# Packages
library(nlme)
library(stargazer)
# We first Calculate the nested model comparisons
loglmod0.nlme=gls(model = logmala~logcd19,
data = dat)
logLMMwREint.nlme=lme(fixed=logmala~1 + logcd19,
random=~1|subject.no,
data = dat)
logLMMwREslope.nlme=lme(fixed=logmala~1 + logcd19,
random=~1+logcd19|subject.no,
data = dat)
## Model 0 < Model 1
anova(loglmod0, logLMwFEint)
anova.lme(loglmod0.nlme, logLMMwREint.nlme, logLMMwREslope.nlme)
## Model 0 < Model 1
anova(loglmod0, logLMwFEint)
## Model 0 < Model 3
anova.lme(loglmod0.nlme,logLMMwREint.nlme)
## Model 0 < Model 3
x=anova.lme(loglmod0.nlme,logLMMwREint.nlme)
names(x)
## Model 3 < Model 4
anova.lme(logLMMwREint.nlme,logLMMwREslope.nlme)
anova(loglmod0, logLMwFEint)
# ResultStatsCD34Fbln.R
# Nested Model Comparisons and Percent Change Matrices
# Load Data
load("/Users/lee/Google Drive/RBC Project/Data and Scripts/FinalData/CompiledDataEnvironments/CD34FblnWS.RData")
# Packages
library(nlme)
library(stargazer)
# We first Calculate the nested model comparisons
loglmod0.nlme=gls(model = logfbln~logcd34,
data = dat)
logLMMwREint.nlme=lme(fixed=logfbln~1 + logcd34,
random=~1|subject.no,
data = dat)
logLMMwREslope.nlme=lme(fixed=logfbln~1 + logcd34,
random=~1+logcd34|subject.no,
data = dat)
## Model 0 < Model 1
anova(loglmod0, logLMwFEint)
## Model 0 < Model 3
anova.lme(loglmod0.nlme,logLMMwREint.nlme)
## Model 3 < Model 4
anova.lme(logLMMwREint.nlme,logLMMwREslope.nlme)
rm(list=ls())
rm(list=ls())
# ResultStatsCD19Mala.R
# Nested Model Comparisons and Percent Change Matrices
# Load Data
load("/Users/lee/Google Drive/RBC Project/Data and Scripts/FinalData/CompiledDataEnvironments/CD19MalaWS.RData")
# Packages
library(nlme)
library(stargazer)
# We first Calculate the nested model comparisons
loglmod0.nlme=gls(model = logmala~logcd19,
data = dat)
logLMMwREint.nlme=lme(fixed=logmala~1 + logcd19,
random=~1|subject.no,
data = dat)
logLMMwREslope.nlme=lme(fixed=logmala~1 + logcd19,
random=~1+logcd19|subject.no,
data = dat)
## Model 0 < Model 1
anova(loglmod0, logLMwFEint)
## Model 0 < Model 3
x=anova.lme(loglmod0.nlme,logLMMwREint.nlme)
## Model 3 < Model 4
anova.lme(logLMMwREint.nlme,logLMMwREslope.nlme)
# Define Percent Change Function
PerctChange=function(bef,aft){
diff=aft-bef
out=round(diff/((aft+bef)/2), digits = 4)
return(out)
}
# Function to calculate Percent Change Matrices
PerctChangeMatrix=function(vec){
len=length(vec)
out.mat=matrix(NA, nrow = len, ncol = len)
for(i in 1:len){
for(j in 1:len){
out.mat[i,j]=PerctChange(vec[i], vec[j])
}
}
return(out.mat)
}
# Create Coefficient Vector:
coef.loglmod=4.918*10**(-2)
coef.logLMwFEint=4.833*10**(-2)
coef.logLMMwREint=4.92*10**(-2)
coef.logLMMwREslope=5.938*10**(-2)
coef.logGEE=2.22*10**(-1)
# Calculate Coeficient percentage of change
coef.CD19Mala=c(coef.loglmod,coef.logLMwFEint,coef.logLMMwREint,
coef.logLMMwREslope,coef.logGEE)
(perctChange.coefCD19Mala=PerctChangeMatrix(coef.CD19Mala))
model.names=c("Mod 0", "Mod 1", "Mod 2", "Mod 3", "Mod 4")
colnames(perctChange.coefCD19Mala)=model.names
rownames(perctChange.coefCD19Mala)=model.names
perctChange.coefCD19Mala.df=data.frame(perctChange.coefCD19Mala)
# Create Standard Error Vector
se.loglmod        =1.455*10 **(-2)
se.logLMwFEint    =1.381*10 **(-2)
se.logLMMwREint   =1.374*10  **(-2)
se.logLMMwREslope =3.538*10 **(-2)
se.logGEE         =6.3*10  **(-1)
# Calculate SE Percent Change matrix
se.CD19Mala=c(se.loglmod,
se.logLMwFEint,
se.logLMMwREint,
se.logLMMwREslope,
se.logGEE)
(perctChange.seCD19Mala=PerctChangeMatrix(se.CD19Mala))
View(GEEmod)
GEEmod[["coefficients"]][["(Intercept)"]]
GEEmod[["coefficients"]][["cd19"]]
GEEmod[["std.err"]]
summary(GEEmod)
GEEmods=summary(GEEmod)
GEEmods$coefficients
lmods
View(lmod0s)
lmod0s$coefficients
GEEmods$coefficients[2,1]
GEEmods$coefficients[2,1]-lmod0s$coefficients[2,1]
GEEmods$coefficients[2,2]-lmod0s$coefficients[2,2]
GEEmods$coefficients[2,3]-lmod0s$coefficients[2,3]
GEEmods$coefficients[2,4]-lmod0s$coefficients[2,4]
(GEEmods$coefficients[2,4]-lmod0s$coefficients[2,4])/GEEmods$coefficients[2,4]
## Model 0 < Model 1
anova(loglmod0, logLMwFEint)
## Model 0 < Model 1
x= anova(loglmod0, logLMwFEint)
x$`Pr(>F)`
x$F
x$Df
x$Df
x$Res.Df
## Model 0 < Model 3
x=anova.lme(loglmod0.nlme,logLMMwREint.nlme)
x$`p-value`
x
## Model 3 < Model 4
x=anova.lme(logLMMwREint.nlme,logLMMwREslope.nlme)
x$`p-value`
# ResultStatsCD34Fbln.R
# Nested Model Comparisons and Percent Change Matrices
# Load Data
load("/Users/lee/Google Drive/RBC Project/Data and Scripts/FinalData/CompiledDataEnvironments/CD34FblnWS.RData")
# Packages
library(nlme)
library(stargazer)
# We first Calculate the nested model comparisons
loglmod0.nlme=gls(model = logfbln~logcd34,
data = dat)
logLMMwREint.nlme=lme(fixed=logfbln~1 + logcd34,
random=~1|subject.no,
data = dat)
logLMMwREslope.nlme=lme(fixed=logfbln~1 + logcd34,
random=~1+logcd34|subject.no,
data = dat)
## Model 0 < Model 1
x=anova(loglmod0, logLMwFEint)
## Model 0 < Model 3
x=anova.lme(loglmod0.nlme,logLMMwREint.nlme)
## Model 3 < Model 4
x=anova.lme(logLMMwREint.nlme,logLMMwREslope.nlme)
# Define Percent Change Function
PerctChange=function(bef,aft){
diff=aft-bef
out=round(diff/((aft+bef)/2), digits = 4)
return(out)
}
# Function to calculate Percent Change Matrices
PerctChangeMatrix=function(vec){
len=length(vec)
out.mat=matrix(NA, nrow = len, ncol = len)
for(i in 1:len){
for(j in 1:len){
out.mat[i,j]=PerctChange(vec[i], vec[j])
}
}
return(out.mat)
}
# Create Coefficient Vector:
coef.loglmod=7.884*10**(-1)
coef.logLMwFEint=1.306*10**(-1)
coef.logLMMwREint=1.35*10**(-1)
coef.logLMMwREslope=1.705*10**(-1)
coef.logGEE=7.88*10**(-1)
# Calculate Coeficient percentage of change
coef.CD34Fbln=c(coef.loglmod,coef.logLMwFEint,coef.logLMMwREint,
coef.logLMMwREslope,coef.logGEE)
(perctChange.coef.CD34Fbln=PerctChangeMatrix(coef.CD34Fbln))
model.names=c("Mod 0", "Mod 1", "Mod 2", "Mod 3", "Mod 4")
colnames(perctChange.coef.CD34Fbln)=model.names
rownames(perctChange.coef.CD34Fbln)=model.names
perctChange.coef.CD34Fbln.df=data.frame(perctChange.coef.CD34Fbln)
# Create Standard Error Vector
se.loglmod        =4.92*10**(-2)
se.logLMwFEint    =3.42*10**(-2)
se.logLMMwREint   =3.42*10**(-2)
se.logLMMwREslope =7.29*10**(-2)
se.logGEE         =4.69*10**(-1)
# Calculate SE Percent Change matrix
se.CD34Fbln=c(se.loglmod,
se.logLMwFEint,
se.logLMMwREint,
se.logLMMwREslope,
se.logGEE)
(perctChange.seCD34Fbln=PerctChangeMatrix(se.CD34Fbln))
## Model 0 < Model 1
x=anova(loglmod0, logLMwFEint)
x$`Pr(>F)`
## Model 0 < Model 3
x=anova.lme(loglmod0.nlme,logLMMwREint.nlme)
x$`p-value`
## Model 3 < Model 4
x=anova.lme(logLMMwREint.nlme,logLMMwREslope.nlme)
x
x$`p-value`
knit_with_parameters('~/Documents/GitHub/MSproject_RBC/MSproject_RBC/FinalizedWriteUps/vFINAL/5 Results/ResultsVFinal.Rmd')
Here, I compare five methods for modeling scRNA-seq expression profiles that account for within-subject correlation: linear modeling (LM), linear modeling with subjects as fixed effects (LM-FE), linear mixed effects models with subjects as random intercepts (LMM-RI) and random slopes (LMM-RS), and generalized estimating equations. I will present the framework for each method in the context of a generalized predictor-response pairing.  I will asses each model's estimate of the fixed effect slope parameter for stability across modeling approach using subject-correlated single-cell data from a study of 27 Lupus Nephritis cases.  We will also evaluate standard errors and test statistics for this parameter.
####	functionsProbScoreCalc.R	 ####
####	Description:	 ####
# This script will define the following functions that will evaluate a subjects answer for a probabilistic fit.
#-------------------------------------------------------------------------#
####   Generalizes over training/test data pairings
## EvalSeqData=function(cvData.train.in, cvData.test.in, cvData.N.set.in)
####  Generalizes over multiple subjects in a single test/train pairing
## EvalSeqSubjec=function(train.weights.in, test.data.in, set.no)
####	Evaluates sequence for classification convergence for one subject in a single train/test pairing
## EvalSeq=function(train.weights.in, test.data.in, set.no, subject.no)
#-------------------------------------------------------------------------#
####	Script Dependencies	 ####
# Package Dependencies:
# Set Working Directory
WD="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis"
setwd(WD)
# Data Dependencies:
# PHQ9 Data
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/DataManagement/PHQ9/functionsDataImportProcessing.R")
# Variable Dependencies:
set.seed(123)
# File Dependencies
## functions:
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsWeightCalculations.R")
####	Divide Data into 4 CV data sets
CV4.dat=CVsplit(full.data, 4)
####	MasterCVanalysis.R	 ####
#-------------------------------------------------------------------------#
####	Description:	 ####
# This script will combine the constituent functions, data, and variables needed in order to perform a cross-validation examination of Probabilistic Scoring.
####	Script Dependencies	 ####
# Package Dependencies:
library(ggplot2)
library(reshape)
# Set Working Directory
WD="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis"
setwd(WD)
# Data Dependencies:
# PHQ9 Data
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/DataManagement/PHQ9/phq9DataSubsetImport.R")
# Variable Dependencies:
set.seed(123)
options(warn = -1)
# File Dependencies
## functions:
# Weight Calculations
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsWeightCalculations.R")
# Probabilistic Score Calculations
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsProbScoreCalc.R")
# Scoring Analysis
source(file="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsScoringAnalysis.R")
#-------------------------------------------------------------------------#
####	Begin Script	 ####
#-------------------------------------------------------------------------#
####	CV-meta analysis	 ####
####  Determine the number of training observations for each K-set selection
number.of.training.obs=c()
number.of.sets=3:2493
for(i in 3:2493){
number.of.training.obs[i-2]=(floor(2495/number.of.sets[i-2]))*(number.of.sets[i-2]-1)
}
plot(number.of.training.obs~number.of.sets)
####  Determine min and max number of training observations possible
min.train.obs=min(number.of.training.obs)
max.train.obs=max(number.of.training.obs)
####  Determine which k-value correspond to the extemas
number.of.sets.min=number.of.sets[which.min(number.of.training.obs)]
number.of.sets.max=number.of.sets[which.max(number.of.training.obs)]
#-------------------------------------------------------------------------#
#### full data weight calculations	 ####
#-------------------------------------------------------------------------#
full.data=phq9
full.data.weights=ReformatWeights(PCVeval_overQnum(full.data))
for(i in 1:3){
full.data.weights[[i]]=round(full.data.weights[[i]], digits = 4)
}
####  Calculate full data-weight probabilistic outcomes
full.data.probSequences=EvalSeqSubject(full.data.weights, full.data, 1)
full.data.probClasses=c()
full.data.probClasses.Convg=list()
for(i in 1:2495){
full.data.probClasses.Convg[[i]]=convg(full.data.probSequences[[i]], 0.66)
}
for(i in 1:2495){
full.data.probClasses[i]=full.data.probClasses.Convg[[i]][[3]]
}
full.data.accuracy=length(which(full.data.probClasses==full.data$SupOutNum))/2495
#-------------------------------------------------------------------------#
#### Accuracy as a function of threshold specification, for full data set	 ####
#-------------------------------------------------------------------------#
thresholdAccuracy=function(accuracy.in, data.weights.in, data.set.in){
init.accuracy.in=accuracy.in
init.data.weights.in=data.weights.in
init.data.set.in=data.set.in
data.probClasses=c()
data.probClasses.Convg=list()
rowdim.data.set.in=dim(init.data.set.in)[1]
data.probSequences=EvalSeqSubject(init.data.weights.in, init.data.set.in, 1)
for(i in 1:rowdim.data.set.in){
data.probClasses.Convg[[i]]=convg(data.probSequences[[i]], init.accuracy.in)
}
for(i in 1:rowdim.data.set.in){
data.probClasses[i]=data.probClasses.Convg[[i]][[3]]
}
out.data.accuracy=length(which(data.probClasses==init.data.set.in$SupOutNum))/rowdim.data.set.in
return(out.data.accuracy)
}
thresholdAccuracy.vector_fulldata=c()
accuracy.threshold.argument=seq(from=0.35, to = 0.95, by = 0.025)
for(i in 1:25){
thresholdAccuracy.vector_fulldata[i]=thresholdAccuracy(accuracy.threshold.argument[i],
full.data.weights,
full.data)
}
plot(thresholdAccuracy.vector_fulldata~accuracy.threshold.argument)
thresholdAccuracy.vector_fulldata[17]
accuracy.threshold.argument[17]
# 0.75 is the highest classification accuracy argument
#-------------------------------------------------------------------------#
####	CV 4	 ####
#-------------------------------------------------------------------------#
####	Divide Data into 4 CV data sets
CV4.dat=CVsplit(full.data, 4)
View(CV4.dat)
View(full.data.weights)
full.data.weights[[1]]
full.data.weights[[1]]
full.data.weights
