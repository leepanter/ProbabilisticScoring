plot(df$totshootstate~df$GunOwners, ylim=c(-50,450), xlab="Perc Pop Gun Owners", ylab="Total FPS Incidences",
main="% Pop Gun Owners vs State Population")
with(df, text(df$totshootstate~df$GunOwners, labels = row.names(df), pos = 1))
plot(df$totshootstate~df$Under45, ylim=c(-50,450), xlab="Perc Pop Under 45", ylab="Total FPS Incidences",
main="% Pop Under 45 vs State Population")
with(df, text(df$totshootstate~df$Under45, labels = row.names(df), pos = 1))
plot(df$totshootstate~df$PercMale, ylim=c(-50,450), xlab="Males per 100 females", ylab="Total FPS Incidences",
main="Males per 100 females vs State Population")
with(df, text(df$totshootstate~df$PercMale, labels = row.names(df), pos = 1))
#############################################################################################
#############################################################################################
#Create Covariance Scaled Data Frame
#############################################################################################
df
lmodi=lm(totshootstate~StatePop,data = df)
(lmods=summary(lmodi))
lmodi2=lm(StatePop~.-totshootstate, data = df)
(lmodi2s=summary(lmodi2))
df2=df
df2$scaled.shot=NA
for(i in 1:51)
{
df2$scaled.shot[i]=abs(df2$totshootstate[i]-df2$StatePop[i])
}
df2$scaled.shot
#############################################################################################
#############################################################################################
#Modeling,Selection, and Collinearily
#############################################################################################
lmod1=lm(scaled.shot~Poverty+HS+White+Black+Asian+NativeAmerican+Hispanic+Other+NumMurders+
GunOwners+under45+PercMale,data=df2)
(lmod1s=summary(lmod1))
vif(lmod1)
#remove Asian
lmod1=lm(scaled.shot~Poverty+HS+White+Black+NativeAmerican+Hispanic+Other+NumMurders+
GunOwners+under45+PercMale,data=df2)
(lmod1s=summary(lmod1))
vif(lmod1)
#Remove White
lmod1=lm(scaled.shot~Poverty+HS+Black+NativeAmerican+Hispanic+Other+NumMurders+
GunOwners+under45+PercMale,data=df2)
(lmod1s=summary(lmod1))
vif(lmod1)
#Remove GunOwners
lmod1=lm(scaled.shot~Poverty+HS+Black+NativeAmerican+Hispanic+Other+NumMurders
+under45+PercMale,data=df2)
(lmod1s=summary(lmod1))
vif(lmod1)
#Remove Black
lmod1=lm(scaled.shot~Poverty+HS+NativeAmerican+Hispanic+Other+NumMurders
+under45+PercMale,data=df2)
(lmod1s=summary(lmod1))
vif(lmod1)
#Nothing to remove
lmod1=lm(scaled.shot~HS+Hispanic
,data=df2)
(lmod1s=summary(lmod1))
vif(lmod1)
lmodf=lm(scaled.shot~HS+Hispanic, data = df2)
lmodfs=summary(lmodf)
#############################################################################################
#############################################################################################
#Checking Model Assumptions
#############################################################################################
#Constant Variance
plot(fitted(lmodf),residuals(lmodf),xlab = "Fitted", ylab="Residuals")
abline(h=0)
lmodfsqrt=lm(sqrt(scaled.shot)~HS+Hispanic, data = df2)
lmodfsqrts=summary(lmodfsqrt)
plot(fitted(lmodfsqrt),residuals(lmodfsqrt),xlab = "Fitted", ylab="Residuals")
abline(h=0)
#Normality
qqnorm(residuals(lmodfsqrt),ylab="Residuals")
qqline(residuals(lmodfsqrt))
hist(residuals(lmodfsqrt), xlab="Residuals")
#Uncorrelated Errors
n=length(residuals(lmodfsqrt))
plot(tail(residuals(lmodfsqrt),n-1)~head(residuals(lmodfsqrt),n-1),
xlab=expression(hat(epsilon)[i]), ylab=expression(hat(epsilon)[i+1]))
abline(h=0,v=0)
df.reg=data.frame(df2,data_State$Region)
levels(df.reg$data_State.Region)
lmodfsqrt=lm(sqrt(scaled.shot)~HS+Hispanic, data = df.reg)
lmodfsqrts=summary(lmodfsqrt)
region.resid.df=data.frame(data_State$Region, lmodfsqrts$residuals)
mw.states=data_State$SateAbrev[data_State$Region=="MW"]
mw.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="MW"]
n.mw=length(mw.region.resid.df)
plot(tail(mw.region.resid.df,n.mw-1)~head(mw.region.resid.df,n.mw-1),
xlab=expression(hat(epsilon)[i]), ylab=expression(hat(epsilon)[i+1]))
abline(h=0,v=0)
ne.states=data_State$SateAbrev[data_State$Region=="NE"]
ne.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="NE"]
n.ne=length(ne.region.resid.df)
plot(tail(ne.region.resid.df,n.ne-1)~head(ne.region.resid.df,n.ne-1),
xlab=expression(hat(epsilon)[i]), ylab=expression(hat(epsilon)[i+1]))
abline(h=0,v=0)
nw.states=data_State$SateAbrev[data_State$Region=="NW"]
nw.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="NW"]
n.nw=length(nw.region.resid.df)
plot(tail(nw.region.resid.df,n.nw-1)~head(nw.region.resid.df,n.nw-1),
xlab=expression(hat(epsilon)[i]), ylab=expression(hat(epsilon)[i+1]))
abline(h=0,v=0)
se.states=data_State$SateAbrev[data_State$Region=="SE"]
se.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="SE"]
n.se=length(se.region.resid.df)
plot(tail(se.region.resid.df,n.se-1)~head(se.region.resid.df,n.se-1),
xlab=expression(hat(epsilon)[i]), ylab=expression(hat(epsilon)[i+1]))
abline(h=0,v=0)
sw.states=data_State$SateAbrev[data_State$Region=="SW"]
sw.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="SW"]
n.sw=length(sw.region.resid.df)
plot(tail(sw.region.resid.df,n.sw-1)~head(sw.region.resid.df,n.sw-1),
xlab=expression(hat(epsilon)[i]), ylab=expression(hat(epsilon)[i+1]))
abline(h=0,v=0)
#Leverage Values
lmod=lm(sqrt(scaled.shot)~HS+Hispanic, data = df2)
lmods=summary(lmod)
hatv=hatvalues(lmod)
states=row.names(df2)
halfnorm(hatv,labs=states,ylab = "Leverages")
qqnorm(rstandard(lmod))
abline(0,1)
#outliers
outlierTest(lmod)
df3=df2[-32,]
lmod.NJ=lm(sqrt(scaled.shot)~HS+Hispanic, data = df3)
lmod.NJs=summary(lmod.NJ)
perc.change=c()
for(i in 1:3)
{
perc.change[i]=(lmods$coefficients[i]-lmod.NJs$coefficients[i])/lmods$coefficients[i]
}
#Influential Observations
cook=cooks.distance(lmod)
halfnorm(cook,labs = states,ylab ="Cook's distances")
r1=range(cook)[2]-range(cook)[1]
cook.nj=cooks.distance(lmod.NJ)
halfnorm(cook.nj,labs = states,ylab ="Cook's distances")
r2=range(cook.nj)[2]-range(cook.nj)[1]
perc.change=(r1-r2)/r1
#############################################################################################
#final model
lmod=lm(sqrt(scaled.shot)~HS+Hispanic, data = df2)
lmods=summary(lmod)
#############################################################################################
#Standardized Model
#############################################################################################
totshootstate.std=(df$totshootstate-mean(df$totshootstate))/sd(df$totshootstate)
statepop.std=(df$StatePop-mean(df$StatePop))/sd(df$StatePop)
scaled.shot.std=abs(totshootstate.std-statepop.std)
HS.std=(df2$HS-mean(df2$HS))/sd(df2$HS)
Hispanic.std=(df2$Hispanic-mean(df2$Hispanic))/sd(df2$Hispanic)
df.std=data.frame(scaled.shot.std,HS.std,Hispanic.std)
lmod.std=lm(scaled.shot.std~HS.std+Hispanic.std, data=df.std)
lmod.stds=summary(lmod.std)
#General State Information (State Abbreviations, Region, Official Name,...etc)
data_State=read.csv("/Users/lee/Desktop/FINAL PROJECT/Project_Data_Official/State_Data.csv")
#Washington Post Shooting Data
data_shootings=read.csv("/Users/lee/Desktop/FINAL PROJECT/Project_Data_Official/fatal-police-shootings-in-the-us/PoliceKillingsUS_Rev1.csv")
#US Census State-Poverty Data
data_poverty=read.csv("/Users/lee/Desktop/FINAL PROJECT/Project_Data_Official/Census_Poverty_Data/Poverty_Data.csv")
#US Census State-High School Graduation Data
data_HS=read.csv("/Users/lee/Desktop/FINAL PROJECT/Project_Data_Official/Census_HS_Data/Census_HS_Data_Rev.csv")
#US Census State-Race Composition Data
data_Race=read.csv("/Users/lee/Desktop/FINAL PROJECT/Project_Data_Official/Census_Race_Data/Census_Race_Data_Rev.csv")
#US Census State-Population Composition
data_population=read.csv("/Users/lee/Desktop/FINAL PROJECT/Project_Data_Official/Census_Population_Data/Census_Data_Populations.csv")
#US Census State-Age Composition
data_age=read.csv("/Users/lee/Desktop/FINAL PROJECT/Project_Data_Official/Census_Age_Data/Census_age_data.csv")
#Violent Crime data
data_ViolentCrime=read.csv("/Users/lee/Desktop/FINAL PROJECT/Project_Data_Official/Violent_Crime_Stats.csv")
#############################################################################################
#Creating Covariate Matrix
#############################################################################################
#create "Other" race classification
data_Race_Other=data_Race$PacificIslander+data_Race$TwoorMore+data_Race$Other
#Store Values related to Violent Crime
num.murders=data_ViolentCrime$Gun.Murder.Rate.per.100K..2010.
gun.owners=data_ViolentCrime$Gun.Ownership..2007.
#Store values related to age
under45=c()
for(i in 1:51)
for(i in 1:51)
{
under45[i]=data_age$Under.18[i]+data_age$X18.to.24[i]+data_age$X25.to.44[i]
}
#Store values related to gender
perc.male=c()
for(i in 1:51)
{
perc.male[i]=data_age$Males.per.100.females...All.ages[i]
}
#Create Covariate Matrix
Covariates=data.frame(data_State$SateAbrev, data_population$Population, data_poverty$Percent,data_HS$Percent,
data_Race$White,data_Race$BlackOnly,data_Race$Asian,data_Race$NativeAmerican,
data_Race$Hispanic,data_Race_Other, num.murders, gun.owners, under45, perc.male)
names(Covariates)=c("StateAbbrev","StatePop","Poverty","HS","White","Black","Asian","NativeAmerican",
"Hispanic","Other","NumMurders","GunOwners", "Under45", "PercMale")
#############################################################################################
#Data Cleaning WPPSD
#############################################################################################
#Create Age Groups
age=data_shootings$age
ageNA=unknownToNA(x=age, unknown=c("",NA,0))
data_shootings$AgeGroup<-NA
data_shootings$AgeGroup[ageNA<=9]<-"0s"
data_shootings$AgeGroup[ageNA>=10 & ageNA<=19]<-"10s"
data_shootings$AgeGroup[ageNA>=20 & ageNA<=29]<-"20s"
data_shootings$AgeGroup[ageNA>=30 & ageNA<=39]<-"30s"
data_shootings$AgeGroup[ageNA>=40 & ageNA<=49]<-"40s"
data_shootings$AgeGroup[ageNA>=50 & ageNA<=59]<-"50s"
data_shootings$AgeGroup[ageNA>=60 & ageNA<=69]<-"60s"
data_shootings$AgeGroup[ageNA>=70 & ageNA<=79]<-"70s"
data_shootings$AgeGroup[ageNA>=80 & ageNA<=89]<-"80s"
data_shootings$AgeGroup[ageNA>=90 & ageNA<=99]<-"90s"
data_shootings$AgeGroup[is.na(data_shootings$age) | data_shootings$age==""]<-"Unknown"
#Rename Race & Gender Levels
data_shootings$race=unknownToNA(x=data_shootings$race, unknown=c("",NA,0))
data_shootings$race=NAToUnknown(x=data_shootings$race, unknown = "Unknown")
levels(data_shootings$race)=c("Asian", "Black", "Hispanic", "NativeAmerican", "Other", "Unknown","White")
levels(data_shootings$gender)=c("Female","Male")
#Store Critical Values
CaseNO=data_shootings$CaseNO
AgeGroup=data_shootings$AgeGroup
Gender=data_shootings$gender
Race=data_shootings$race
State=data_shootings$state
#############################################################################################
#Setting Data Types
#############################################################################################
#Covariates DF
Covariates$StateAbbrev=factor(Covariates$StateAbbrev)
#Shootings DF
data_shootings$gender=factor(data_shootings$gender)
data_shootings$race=factor(data_shootings$race)
data_shootings$state=factor(data_shootings$state)
data_shootings$AgeGroup=factor(data_shootings$AgeGroup)
#############################################################################################
#Create Rawdata Data Frame
#############################################################################################
RawData=data.frame(CaseNO, AgeGroup, Gender, Race, State)
#Modify RawData Dataframe with Poverty Covariate
RawData$Poverty=NA
for(i in 1:2535)
{
RawData$Poverty[i]=Covariates$Poverty[RawData$State[i]==Covariates$StateAbbrev]
}
#Modify RawData Dataframe with HS Covariate
RawData$HS=NA
for(i in 1:2535)
{
RawData$HS[i]=Covariates$HS[RawData$State[i]==Covariates$StateAbbrev]
}
#Modify RawData Dataframe with Race Variables
RawData$White=NA
RawData$Black=NA
RawData$Asian=NA
RawData$NativeAmerican=NA
RawData$Hispanic=NA
RawData$Other=NA
for(i in 1:2535)
{
RawData$White[i]=Covariates$White[RawData$State[i]==Covariates$StateAbbrev]
RawData$Black[i]=Covariates$Black[RawData$State[i]==Covariates$StateAbbrev]
RawData$Asian[i]=Covariates$Asian[RawData$State[i]==Covariates$StateAbbrev]
RawData$NativeAmerican[i]=Covariates$NativeAmerican[RawData$State[i]==Covariates$StateAbbrev]
RawData$Hispanic[i]=Covariates$Hispanic[RawData$State[i]==Covariates$StateAbbrev]
RawData$Other[i]=Covariates$Other[RawData$State[i]==Covariates$StateAbbrev]
}
#Modify RawData Dataframe with Population Covariate
RawData$Pop=NA
for(i in 1:2535)
{
RawData$Pop[i]=Covariates$StatePop[RawData$State[i]==Covariates$StateAbbrev]
}
RawData$Count=1
#############################################################################################
#Create Count Data, in FPS data frame
#############################################################################################
FPS=aggregate(RawData$Count, by=list(State=RawData$State, AgeGroup=RawData$AgeGroup,
Race=RawData$Race,Gender=RawData$Gender), FUN=sum)
FPS=data.frame(FPS$x,FPS$State,FPS$AgeGroup,FPS$Race,FPS$Gender)
names(FPS)=c("Shoot","State","AgeGroup","Race","Gender")
totShootState=tapply(FPS$Shoot,FPS$State,sum)
totShootAgeGroup=tapply(FPS$Shoot,FPS$AgeGroup,sum)
totShootRace=tapply(FPS$Shoot,FPS$Race,sum)
totShootGender=tapply(FPS$Shoot,FPS$Gender,sum)
#############################################################################################
#Create Create Final(non-covariate-scaled) Data Frame
#############################################################################################
df.totshootState=data.frame(totShootState)
df=data.frame(df.totshootState,Covariates$StatePop,Covariates$Poverty,
Covariates$HS,Covariates$White,Covariates$Black,Covariates$Asian,
Covariates$NativeAmerican,Covariates$Hispanic,Covariates$Other,
Covariates$NumMurders,Covariates$GunOwners,Covariates$Under45,
Covariates$PercMale)
names(df)=c("totshootstate","StatePop","Poverty","HS","White","Black",
"Asian", "NativeAmerican", "Hispanic", "Other", "NumMurders", "GunOwners",
"Under45", "PercMale")
#############################################################################################
#Modeling, and selection
#############################################################################################
lmodi=lm(totshootstate~StatePop,data = df)
lmods=summary(lmodi)
lmodi2=lm(StatePop~.-totshootstate, data = df)
lmodi2s=summary(lmodi2)
df2=df
df2$scaled.shot=NA
for(i in 1:51)
{
df2$scaled.shot[i]=abs(df2$totshootstate[i]-df2$StatePop[i])
}
lmod1=lm(scaled.shot~Poverty+HS+White+Black+Asian+NativeAmerican+Hispanic+Other+NumMurders+
GunOwners+under45+PercMale,data=df2)
lmod1s=summary(lmod1)
lmod1=lm(scaled.shot~Poverty+HS+NativeAmerican+Hispanic+Other+NumMurders
+under45+PercMale,data=df2)
lmod1s=summary(lmod1)
lmodf=lm(scaled.shot~HS+Hispanic, data = df2)
lmodfs=summary(lmodf)
#############################################################################################
#Model Diagnostics
#############################################################################################
lmodfsqrt=lm(sqrt(scaled.shot)~HS+Hispanic, data = df2)
lmodfsqrts=summary(lmodfsqrt)
#Uncorrelated Errors
n=length(residuals(lmodfsqrt))
df.reg=data.frame(df2,data_State$Region)
lmodfsqrt=lm(sqrt(scaled.shot)~HS+Hispanic, data = df.reg)
lmodfsqrts=summary(lmodfsqrt)
region.resid.df=data.frame(data_State$Region, lmodfsqrts$residuals)
mw.states=data_State$SateAbrev[data_State$Region=="MW"]
mw.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="MW"]
n.mw=length(mw.region.resid.df)
ne.states=data_State$SateAbrev[data_State$Region=="NE"]
ne.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="NE"]
n.ne=length(ne.region.resid.df)
nw.states=data_State$SateAbrev[data_State$Region=="NW"]
nw.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="NW"]
n.nw=length(nw.region.resid.df)
se.states=data_State$SateAbrev[data_State$Region=="SE"]
se.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="SE"]
n.se=length(se.region.resid.df)
sw.states=data_State$SateAbrev[data_State$Region=="SW"]
sw.region.resid.df=region.resid.df$lmodfsqrts.residuals[region.resid.df$data_State.Region=="SW"]
n.sw=length(sw.region.resid.df)
#Leverage Values
lmod=lm(sqrt(scaled.shot)~HS+Hispanic, data = df2)
lmods=summary(lmod)
hatv=hatvalues(lmod)
states=row.names(df2)
#outliers
df3=df2[-32,]
lmod.NJ=lm(sqrt(scaled.shot)~HS+Hispanic, data = df3)
lmod.NJs=summary(lmod.NJ)
perc.change=c()
for(i in 1:3)
{
perc.change[i]=(lmods$coefficients[i]-lmod.NJs$coefficients[i])/lmods$coefficients[i]
}
#final model
lmod=lm(sqrt(scaled.shot)~HS+Hispanic, data = df2)
lmods=summary(lmod)
scaled.shot.std=(df2$scaled.shot-mean(df2$scaled.shot))/sd(df2$scaled.shot)
HS.std=(df2$HS-mean(df2$HS))/sd(df2$HS)
Hispanic.std=(df2$Hispanic-mean(df2$Hispanic))/sd(df2$Hispanic)
df.std=data.frame(scaled.shot.std,HS.std,Hispanic.std)
lmod.std=lm(scaled.shot.std~HS.std+Hispanic.std, data=df.std)
lmod.stds=summary(lmod.std)
lmod1=lm(scaled.shot~Poverty+HS+NativeAmerican+Hispanic+Other+NumMurders
+under45+PercMale,data=df2)
####	MasterCVanalysis.R	 ####
#-------------------------------------------------------------------------#
####	Description:	 ####
# This script will combine the constituent functions, data, and variables needed in order to perform a cross-validation examination of Probabilistic Scoring.
####	Script Dependencies	 ####
# Package Dependencies:
library(ggplot2)
library(reshape)
# Set Working Directory
WD="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis"
setwd(WD)
# Data Dependencies:
# PHQ9 Data
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/DataManagement/PHQ9/phq9DataSubsetImport.R")
# source(file="phq9DataSubsetImport.R")
# Variable Dependencies:
set.seed(123)
options(warn = -1)
## File Dependencies
# CVInitialSetup.R
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/CVInitialSetups.R")
# source(file="CVInitialSetups.R")
## functions:
# Weight Calculations
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsWeightCalculations.R")
# source(file="functionsWeightCalculations.R")
# Probabilistic Score Calculations
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsProbScoreCalc.R")
# source(file="functionsProbScoreCalc.R")
# Scoring Analysis
source(file="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsScoringAnalysis.R")
# source(file="functionsScoringAnalysis.R")
#-------------------------------------------------------------------------#
####	Begin Script	 ####
#-------------------------------------------------------------------------#
# Initialize Empty Variables in Global Scope
accuracy.ksets=c()
traditional.accuracy.ksets=c()
N.obs.k=c()
boot.sample.i=list()
number.samples=100
sample.length=number.samples+2
df.set.info=df.train.set.info
colnames(df.set.info)=c("df.k.sets", "N.obs.train.set")
sample.vec.k.sets=df.set.info$df.k.sets
sample.vec.k.sets=sample.vec.k.sets[-c(1,1245)]
N.set.arg=sort(sample(sample.vec.k.sets, number.samples, replace = FALSE))
N.set.arg=sort(c(N.set.arg,1247, 1248))
# df.set.info=df.k.final
# colnames(df.set.info)=c("df.k.sets", "N.obs.train", "N.obs.test" )
#
# sample.length=100
# N.set.arg=df.set.info$df.k.sets
for(i in 1:sample.length){
boot.sample.i[[i]]=CVsplit(phq9, N.set.arg[i])
}
for(k in 1:sample.length){
k.setVal=N.set.arg[k]
k.index=k
####	Divide Data into K CV data sets
CVk.dat=boot.sample.i[[k.index]]
Number.k.obs=CVk.dat[[3]]
####  Initialized Empty Variables
CVk_j.train=list()
CVk_j.test=list()
CVk_j.train.weights=list()
CVk_j.data.accuracy=c()
CVk_j.data.accuracy.traditional=c()
for(j in 1:k.setVal){
CVk_j.train[[j]]=CVk.dat[[1]][[j]]
CVk_j.test[[j]]=CVk.dat[[2]][[j]]
CVk_j.probClasses=c()
CVk_j.probClasses.Convg=list()
### Calculate data weights
CVk_j.train.weights[[j]]=ReformatWeights(PCVeval_overQnum(CVk_j.train[[j]]))
for(i in 1:3){
CVk_j.train.weights[[j]][[i]]=round(CVk_j.train.weights[[j]][[i]], digits = 4)
}
###  Calculate CVk-j probabilistic outcomes
CVk_j.probSequences=EvalSeqSubject(CVk_j.train.weights[[j]], CVk_j.test[[j]], 1)
for(i in 1:Number.k.obs){
CVk_j.probClasses.Convg[[i]]=convg(CVk_j.probSequences[[i]], 0.75)
}
for(i in 1:Number.k.obs){
CVk_j.probClasses[i]=CVk_j.probClasses.Convg[[i]][[3]]
}
####  Calculate Accuracy of probablistic classes
CVk_j.data.accuracy[j]=length(which(CVk_j.probClasses==CVk_j.test[[j]]$SupOutNum))/Number.k.obs
####  Calculate Accuracy of traditional Classes
CVk_j.data.accuracy.traditional[j]=length(which(CVk_j.test[[j]]$sumClassNum==CVk_j.test[[j]]$SupOutNum))/Number.k.obs
}
####  Output Accuracy Values
CVk.accuracy=mean(CVk_j.data.accuracy)
accuracy.ksets[k.index]=CVk.accuracy
CVk.accuracy.traditional=mean(CVk_j.data.accuracy.traditional)
traditional.accuracy.ksets[k.index]=CVk.accuracy.traditional
}
for(k in 1:sample.length){
N.obs.k[k]=length(boot.sample.i[[k]][[1]])
}
accuracy.df=data.frame(N.obs.k, accuracy.ksets)
accuracy.lm=lm(accuracy.ksets~N.obs.k, data = accuracy.df)
(accuracy.lms=summary(accuracy.lm))
accuracy.plot=ggplot(accuracy.df, aes(x=N.obs.k, y=accuracy.ksets))+
xlab("Observations in Traing Set")+
ylab("Accuracy of Prob.Scoring")+
geom_abline(intercept = as.numeric(coef(accuracy.lm))[[1]],
slope=as.numeric(coef(accuracy.lm))[[2]])+
geom_point()
accuracy.plot
zeros=rep(0, times=sample.length)
ones=rep(1, times=sample.length)
ID=as.factor(c(ones, zeros))
N.obs.train.k.lmFE=rep(N.obs.k, times=2)
accuracy.out.lmFE=c(accuracy.ksets,traditional.accuracy.ksets)
accuracy.df.lmFE=data.frame(N.obs.train.k.lmFE, ID, accuracy.out.lmFE)
accuracy.lmFE=lm(accuracy.out.lmFE~ID+N.obs.train.k.lmFE+ID:N.obs.train.k.lmFE,
data = accuracy.df.lmFE)
(accuracy.lmFEs=summary(accuracy.lmFE))
accuracy.df.lmFE.re=reshape::melt(accuracy.df.lmFE, id.vars=c("N.obs.train.k.lmFE","ID"))
trad.intercept=as.numeric(coef(accuracy.lmFE))[1]+as.numeric(coef(accuracy.lmFE))[2]
trad.slope=as.numeric(coef(accuracy.lmFE))[3]+as.numeric(coef(accuracy.lmFE))[4]
prob.intercept=as.numeric(coef(accuracy.lmFE))[1]
prob.slope=as.numeric(coef(accuracy.lmFE))[3]
p=ggplot2::ggplot(accuracy.df.lmFE.re, aes(x = N.obs.train.k.lmFE, y = accuracy.out.lmFE,
group = ID))+
geom_point()+
geom_abline(intercept = trad.intercept, slope = trad.slope)+
geom_abline(intercept = prob.intercept, slope = prob.slope)
p
#-------------------------------------------------------------------------#
####	End Script	 ####
#-------------------------------------------------------------------------#
#-------------------------------------------------------------------------#
#####	Post-Script	#####
####  Notes:
####  Compilation Errors:
####  Execution Errors:
####  Next Scripts to Consider:
#-------------------------------------------------------------------------#
