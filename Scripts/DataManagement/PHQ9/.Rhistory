for(j in length.boot.weights[i]){
Pscore.CVk.train.weights.round_sample_class=list()
for(k in 1:3){
Pscore.CVk.train.weights.round_sample_class[[k]]=round(Pscore.CVk.train.weights[[i]][[j]][[k]], digits = 4)
}
Pscore.CVk.train.weights.round_sample[[j]]=Pscore.CVk.train.weights.round_sample_class
}
Pscore.CVk.train.weights.round[[i]]=Pscore.CVk.train.weights.round_sample
}
Pscore.CVK.probSequences=list()
for(i in 1:sample.length){
Pscore.CVK.probSequences_sample=list()
for(j in 1:length.boot.weights[i]){
Pscore.CVK.probSequences_sample[[j]]=EvalSeqSubject(Pscore.CVk.train.weights[[i]][[j]],boot.sample.test.i[[i]][[j]],1)
}
Pscore.CVK.probSequences[[i]]=Pscore.CVK.probSequences_sample
}
Pscore.CVk.probClasses.Convg=list()
unlist.boot.sample.Nset=unlist(boot.sample.Nset.i)
for(i in 1:sample.length){
Pscore.CVk.probClasses.Convg_sample=list()
for(j in 1:length.boot.weights[i]){
Pscore.CVk.probClasses.Convg_sample_k=list()
for(k in 1:unlist.boot.sample.Nset[i]){
Pscore.CVk.probClasses.Convg_sample_k[[k]]=convg(Pscore.CVK.probSequences[[i]][[j]][[k]], 0.75)
}
Pscore.CVk.probClasses.Convg_sample[[j]]=Pscore.CVk.probClasses.Convg_sample_k
}
Pscore.CVk.probClasses.Convg[[i]]=Pscore.CVk.probClasses.Convg_sample
}
Class.out.Pscore=list()
for(i in 1:sample.length){
Class.out.Pscore_sample=list()
for(j in 1:length.boot.weights[i]){
Class.out.Pscore_sample_k=c()
for(k in 1:unlist.boot.sample.Nset[i]){
Class.out.Pscore_sample_k[k]=Pscore.CVk.probClasses.Convg[[i]][[j]][[k]][[3]]
}
Class.out.Pscore_sample[[j]]=Class.out.Pscore_sample_k
}
Class.out.Pscore[[i]]=Class.out.Pscore_sample
}
Class.out.Pscore_sample=list()
Class.out.Sup3_sample=list()
for(i in 1:sample.length){
Class.out.Pscore_sample[[i]]=unlist(Class.out.Pscore[[i]])
Class.out.Sup3_sample[[i]]=unlist(Class.out.Sup3[[i]])
}
trad.class=list()
for(i in 1:52){
trad.class.i=list()
for(j in 1:length(boot.sample.i[[i]][[2]])){
trad.class.i=append(trad.class.i,
boot.sample.i[[i]][[2]][[j]][["sumClassNum"]])
}
trad.class[[i]]=trad.class.i
}
accuracy.Sup3=c()
accuracy.trad=c()
for(i in 1:sample.length){
accuracy.Sup3[i]=length(which(Class.out.Pscore_sample[[i]]==Class.out.Sup3_sample[[i]]))/length(Class.out.Sup3_sample[[i]])
accuracy.trad[i]=length(which(Class.out.Pscore_sample[[i]]==trad.class[[i]]))/length(trad.class[[i]])
}
plot(accuracy.Sup3~length.boot.weights)
plot(accuracy.trad~length.boot.weights)
View(boot.weights.i)
View(boot.sample.i)
length.train.boot.weights=c()
for(i in 1:sample.length){
length.train.boot.weights[i]=dim(boot.sample.i[[i]][[1]][[1]])[1]
}
plot(accuracy.Sup3~length.train.boot.weights)
plot(accuracy.trad~length.train.boot.weights)
write.csv(df, file = "/Users/lee/Desktop/df_acc_100_sup2.csv")
df2=read.csv(file = "/Users/lee/Desktop/df_acc_100_sup22.csv")
df2=read.csv(file = "/Users/lee/Desktop/df_acc_100_sup22.csv")
write.csv(df, file = "/Users/lee/Desktop/df_acc_100_sup2.csv")
df2=read.csv(file = "/Users/lee/Desktop/df_acc_100_sup22.csv")
df2=df
str(df2)
df=data.frame(length.boot.weights, accuracy.Sup3, accuracy.trad)
write.csv(df, file = "/Users/lee/Desktop/df_acc_100_sup2.csv")
df2=read.csv(file = "/Users/lee/Desktop/df_acc_100_sup22.csv")
df2=df
str(df2)
df2$ID=as.factor(df2$ID)
p=ggplot(df, aes(x=length.boot.weights))+
geom_point(aes(y=accuracy.Sup3))+
geom_point(aes(y=accuracy.trad))+
geom_abline(intercept = coef(lmod)[1], slope = coef(lmod)[2])+
geom_abline(intercept = coef(lmod)[1]+coef(lmod)[3], slope = coef(lmod)[2]+coef(lmod)[4])
zeros=rep(0, times=sample.length)
ones=rep(1, times=sample.length)
ID=c(zeros, ones)
len.train=c(length.train.boot.weights,length.train.boot.weights)
zeros=rep(0, times=sample.length)
ones=rep(1, times=sample.length)
ID=c(zeros, ones)
len.train=c(length.train.boot.weights,length.train.boot.weights)
accuracy=c(accuracy.Sup3,accuracy.trad)
df2=data.frame(len.train, as.factor(ID), accuracy)
lmod=lm(accuracy~len.train*ID, data=df2)
(lmods=summary(lmod))
df=data.frame(length.train.boot.weights, accuracy.Sup3, accuracy.trad)
p=ggplot(df, aes(x=length.train.boot.weights))+
geom_point(aes(y=accuracy.Sup3))+
geom_point(aes(y=accuracy.trad))+
geom_abline(intercept = coef(lmod)[1], slope = coef(lmod)[2])+
geom_abline(intercept = coef(lmod)[1]+coef(lmod)[3], slope = coef(lmod)[2]+coef(lmod)[4])
p
plot(accuracy.Sup3~length.boot.weights,
xlab="Training Data Length",
ylab = "Accuracy vs Probabilistically Outcom SUP2")
abline(a=lmods$coefficients[1,1], b=lmods$coefficients[2,1])
p=ggplot(df, aes(x=length.train.boot.weights))+
geom_point(aes(y=accuracy.Sup3))+
geom_point(aes(y=accuracy.trad))+
geom_abline(intercept = coef(lmod)[1], slope = coef(lmod)[2])+
geom_abline(intercept = coef(lmod)[1]+coef(lmod)[3], slope = coef(lmod)[2]+coef(lmod)[4])
p
write.csv(df, file = "/Users/lee/Desktop/df_acc_100_sup2.csv")
# kmeansPscore.R
#-------------------------------------------------------------------------#
####	Description:	 ####
# This script will generate a clustering of the PHQ9 data using a K-means approach.  It will produce three clusters that will then be ordered according to the depression classification ranking based on empirical properties of the clusters.
####	Script Dependencies	 ####
# Package Dependencies:
library(factoextra)
library(tidyverse)
library(cluster)
library(dendextend)
library(ggplot2)
# packageurl <- "https://cran.r-project.org/src/contrib/Archive/kohonen/kohonen_2.0.19.tar.gz"
# install.packages(packageurl, repos = NULL, type = "source")
# library(kohonen)
# Set Working Directory
WD="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis"
# WD="/home/lpanter/PScoreUpdate1/GCloudUpdate"
setwd(WD)
# Data Dependencies:
# PHQ9 Data
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/DataManagement/PHQ9/phq9DataSubsetImport.R")
# source(file = "phq9DataSubsetImport.R")
# Variable Dependencies:
set.seed(123)
options(warn = -1)
## File Dependencies
# CVInitialSetup.R
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/CVInitialSetups.R")
# source(file="CVInitialSetups.R")
## functions:
# Weight Calculations
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsWeightCalculations.R")
# source(file = "functionsWeightCalculations.R")
# Probabilistic Score Calculations
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsProbScoreCalc.R")
# source(file = "functionsProbScoreCalc.R")
# Scoring Analysis
source(file="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsScoringAnalysis.R")
# source(file="functionsScoringAnalysis.R")
#-------------------------------------------------------------------------#
####	Begin Script	 ####
#-------------------------------------------------------------------------#
# load data
dat=phq9
dat.kmeans=dat[,4:12]
# KMEANS Int -> factor function
kmeans.convert=function(in.arg){
if(in.arg==1){
out.arg="C2"
} else if(in.arg==2){
out.arg="C1"
} else{out.arg="C3"}
return(out.arg)
}
pscore.convert=function(in.arg){
out.arg=NA
if(is.na(in.arg)){
out.arg="NA"
} else {
if(in.arg==1){
out.arg="C2"
} else if(in.arg==2){
out.arg="C1"
} else {
out.arg="C3"
}
}
return(out.arg)
}
d=dist(dat.kmeans)
hc1=hclust(d, method = "complete")
plot(as.phylo(hc1), type = "unrooted", cex = 0.6,
no.margin = TRUE)
install.packages("ape")
library(ape)
plot(as.phylo(hc1), type = "unrooted", cex = 0.6,
no.margin = TRUE)
plot(as.phylo(hc),
type = "fan",
tip.color = colors[sub_grp],
label.offset = 1,
cex = 0.7)
plot(as.phylo(hc1),
type = "fan",
tip.color = colors[sub_grp],
label.offset = 1,
cex = 0.7)
# plot(as.phylo(hc1), type = "unrooted", cex = 0.6, no.margin = TRUE)
# plot(hc1)
sub_grp=cutree(hc1, k=3)
colors = c("red", "blue", "green")
plot(as.phylo(hc1),
type = "fan",
tip.color = colors[sub_grp],
label.offset = 1,
cex = 0.7)
plot(as.phylo(hc1),
type = "fan",
tip.color = colors[sub_grp],
cex = 0.7)
plot(as.phylo(hc1), type = "unrooted", cex = 0.6, no.margin = TRUE)
# Hcluster #2 ------------------------------------------------------------#
d2=dist(dat.kmeans, method="manhattan")
hc2=hclust(d2, method = "complete")
sub_grp2=cutree(hc2, k=3)
table(sub_grp2)
colors = c("red", "blue", "green")
plot(as.phylo(hc2),
type = "fan",
tip.color = colors[sub_grp2],
cex = 0.7)
# SOMs.R
#-------------------------------------------------------------------------#
####	Description:	 ####
####	Script Dependencies	 ####
# Package Dependencies:
library(factoextra)
library(tidyverse)
library(cluster)
library(dendextend)
library(ggplot2)
# packageurl <- "https://cran.r-project.org/src/contrib/Archive/kohonen/kohonen_2.0.19.tar.gz"
# install.packages(packageurl, repos = NULL, type = "source")
library(kohonen)
# Set Working Directory
WD="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis"
# WD="/home/lpanter/PScoreUpdate1/GCloudUpdate"
setwd(WD)
# Data Dependencies:
# PHQ9 Data
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/DataManagement/PHQ9/phq9DataSubsetImport.R")
# source(file = "phq9DataSubsetImport.R")
# Variable Dependencies:
set.seed(123)
options(warn = -1)
## File Dependencies
# CVInitialSetup.R
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/CVInitialSetups.R")
# source(file="CVInitialSetups.R")
## functions:
# Weight Calculations
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsWeightCalculations.R")
# source(file = "functionsWeightCalculations.R")
# Probabilistic Score Calculations
source(file = "/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsProbScoreCalc.R")
# source(file = "functionsProbScoreCalc.R")
# Scoring Analysis
source(file="/Users/lee/Documents/GitHub/ProbabilisticScoring/Scripts/cvAnalysis/functionsScoringAnalysis.R")
# source(file="functionsScoringAnalysis.R")
#-------------------------------------------------------------------------#
####	Begin Script	 ####
#-------------------------------------------------------------------------#
# load data
dat=phq9
dat.kmeans=dat[,4:12]
#-------------------------------------------------------------------------#
# SOM Clustering ----------------------------------------------------------
#-------------------------------------------------------------------------#
dat.som=scale(dat.kmeans)
#-------------------------------------------------------------------------#
# 12 X 9 grid
#-------------------------------------------------------------------------#
#plot(som.out108, type = 'changes')
#plot(som.out108)
#plot(som.out108, type = 'count')
#plot(hclust(dist(som.out108$codes)))
# seeds 220
set.seed(2220)
out.grid1=somgrid(xdim = 12, ydim=9, topo = "rectangular")
som.out1=som(dat.som, grid = out.grid1, rlen = 1000)
somOut.1=som.out1$unit.classif
class.cuts1=cutree(hclust(dist(som.out1$codes)),3)
ones1=which(class.cuts1==1)
twos1=which(class.cuts1==2)
threes1=which(class.cuts1==3)
classify.cuts.1=function(in.cut){
if(in.cut %in% ones1){
out.class="C1"
}  else if (in.cut %in% twos1){
out.class="C3"
} else {out.class="C2"}
return(out.class)
}
class.som1=c()
for(i in 1:2495){
class.som1[i]=classify.cuts.1(som.out1$unit.classif[i])
}
dat$class.som1=as.factor(class.som1)
table.som1=dat %>% group_by(class.som1) %>% tally()
table.som1
plot(som.out1, type = 'codes', bgcol = rainbow(3)[class.cuts1])
add.cluster.boundaries(som.out1, class.cuts1)
mean.total.table1=aggregate(x=dat$qTot,
by=list(som.out1[["unit.classif"]]),
FUN=mean)
median.total.table1=aggregate(x=dat$qTot,
by=list(som.out1[["unit.classif"]]),
FUN=median)
#-------------------------------------------------------------------------#
# 12 X 9 grid TAXICAB Metric
#-------------------------------------------------------------------------#
#plot(som.out108, type = 'changes')
#plot(som.out108)
#plot(som.out108, type = 'count')
#plot(hclust(dist(som.out108$codes)))
# seeds 220
set.seed(2220)
out.grid1.tx=somgrid(xdim = 12, ydim=9, topo = "rectangular")
som.out1.tx=som(dat.som, grid = out.grid1.tx, rlen = 1000)
somOut.1.tx=som.out1.tx$unit.classif
class.cuts1.tx=cutree(hclust(dist(som.out1.tx$codes,
method = "manhattan")),3)
ones1.tx=which(class.cuts1.tx==1)
twos1.tx=which(class.cuts1.tx==2)
threes1.tx=which(class.cuts1.tx==3)
classify.cuts.1.tx=function(in.cut){
if(in.cut %in% ones1.tx){
out.class="C1"
}  else if (in.cut %in% twos1.tx){
out.class="C3"
} else {out.class="C2"}
return(out.class)
}
class.som1.tx=c()
for(i in 1:2495){
class.som1.tx[i]=classify.cuts.1.tx(som.out1.tx$unit.classif[i])
}
dat$class.som1.tx=as.factor(class.som1.tx)
table.som1.tx=dat %>% group_by(class.som1.tx) %>% tally()
table.som1.tx
plot(som.out1.tx, type = 'codes', bgcol = rainbow(3)[class.cuts1.tx])
add.cluster.boundaries(som.out1.tx, class.cuts1.tx)
mean.total.table1.tx=aggregate(x=dat$qTot,
by=list(som.out1.tx[["unit.classif"]]),
FUN=mean)
median.total.table1.tx=aggregate(x=dat$qTot,
by=list(som.out1.tx[["unit.classif"]]),
FUN=median)
#-------------------------------------------------------------------------#
set.seed(1110)
out.grid2=somgrid(xdim = 15, ydim= 15, topo = "hexagonal")
som.out2=som(dat.som, grid = out.grid2,
rlen=1000)
somOut2=som.out2$unit.classif
class.cuts2=cutree(hclust(dist(som.out2$codes)),3)
ones2=  which(class.cuts2==1)
twos2=  which(class.cuts2==2)
threes2=which(class.cuts2==3)
classify.cuts2=function(in.cut){
if(in.cut %in% ones2){
out.class="C1"
}  else if (in.cut %in% twos2){
out.class="C3"
} else {out.class="C2"}
return(out.class)
}
class.som2=c()
for(i in 1:2495){
class.som2[i]=classify.cuts2(som.out2$unit.classif[i])
}
dat$class.som2=as.factor(class.som2)
table.som2=dat %>% group_by(class.som2) %>% tally()
table.som2
plot(som.out2, type = 'codes', bgcol = rainbow(3)[class.cuts2])
add.cluster.boundaries(som.out2, class.cuts2)
plot(som.out2, type = 'changes')
mean.total.table2=aggregate(x=dat$qTot,
by=list(som.out2[["unit.classif"]]),
FUN=mean)
median.total.table2=aggregate(x=dat$qTot,
by=list(som.out2[["unit.classif"]]),
FUN=median)
#-------------------------------------------------------------------------#
####  Same as SOM2, but taxicab metric
#-------------------------------------------------------------------------#
set.seed(1110)
out.grid2.tx=somgrid(xdim = 15, ydim= 15, topo = "hexagonal")
som.out2.tx=som(dat.som, grid = out.grid2.tx,
rlen=1000)
somOut2.tx=som.out2.tx$unit.classif
class.cuts2.tx=cutree(hclust(dist(som.out2.tx$codes,
method = "manhattan")),3)
ones2.tx=  which(class.cuts2.tx==1)
twos2.tx=  which(class.cuts2.tx==2)
threes2.tx=which(class.cuts2.tx==3)
classify.cuts2.tx=function(in.cut){
if(in.cut %in% ones2.tx){
out.class="C1"
}  else if (in.cut %in% twos2.tx){
out.class="C3"
} else {out.class="C2"}
return(out.class)
}
class.som2.tx=c()
for(i in 1:2495){
class.som2.tx[i]=classify.cuts2.tx(som.out2.tx$unit.classif[i])
}
dat$class.som2.tx=as.factor(class.som2.tx)
table.som2.tx=dat %>% group_by(class.som2.tx) %>% tally()
table.som2.tx
plot(som.out2.tx, type = 'codes', bgcol = rainbow(3)[class.cuts2.tx])
add.cluster.boundaries(som.out2.tx, class.cuts2.tx)
plot(som.out2.tx, type = 'changes')
mean.total.table2.tx=aggregate(x=dat$qTot,
by=list(som.out2.tx[["unit.classif"]]),
FUN=mean)
median.total.table2.tx=aggregate(x=dat$qTot,
by=list(som.out2.tx[["unit.classif"]]),
FUN=median)
#-------------------------------------------------------------------------#
####  3X9 Mapping
#-------------------------------------------------------------------------#
set.seed(123)
out.grid3=somgrid(xdim = 3, ydim= 9, topo = "rectangular")
som.out3=som(dat.som, grid = out.grid3,
rlen=1000)
# dat$somOut=som.out$unit.classif
somOut3=som.out3$unit.classif
class.cuts3=cutree(hclust(dist(som.out3$codes)),3)
ones3=  which(class.cuts3==1)
twos3=  which(class.cuts3==2)
threes3=which(class.cuts3==3)
classify.cuts3=function(in.cut){
if(in.cut %in% ones3){
out.class="C1"
}  else if (in.cut %in% twos3){
out.class="C3"
} else {out.class="C2"}
return(out.class)
}
class.som3=c()
for(i in 1:2495){
class.som3[i]=classify.cuts3(som.out3$unit.classif[i])
}
dat$class.som3=as.factor(class.som3)
table.som3=dat %>% group_by(class.som3) %>% tally()
table.som3
plot(som.out3, type = 'codes', bgcol = rainbow(3)[class.cuts3])
add.cluster.boundaries(som.out3, class.cuts3)
plot(som.out3, type = 'changes')
mean.total.table3=aggregate(x=dat$qTot,
by=list(som.out3[["unit.classif"]]),
FUN=mean)
median.total.table3=aggregate(x=dat$qTot,
by=list(som.out3[["unit.classif"]]),
FUN=median)
median.total.table3test=aggregate(x=dat$Q1,
by=list(som.out3[["unit.classif"]]),
FUN=median)
median.total.table3test
#-------------------------------------------------------------------------#
####  3X9 Mapping, with taxicab metric
#-------------------------------------------------------------------------#
set.seed(123)
out.grid3.tx=somgrid(xdim = 3, ydim= 9, topo = "rectangular")
som.out3.tx=som(dat.som, grid = out.grid3.tx,
rlen=1000)
somOut3.tx=som.out3.tx$unit.classif
class.cuts3.tx=cutree(hclust(dist(som.out3.tx$codes,
method = "manhattan")),3)
ones3.tx=  which(class.cuts3.tx==1)
twos3.tx=  which(class.cuts3.tx==2)
threes3.tx=which(class.cuts3.tx==3)
classify.cuts3.tx=function(in.cut){
if(in.cut %in% ones3.tx){
out.class="C1"
}  else if (in.cut %in% twos3.tx){
out.class="C3"
} else {out.class="C2"}
return(out.class)
}
class.som3.tx=c()
for(i in 1:2495){
class.som3.tx[i]=classify.cuts3.tx(som.out3.tx$unit.classif[i])
}
dat$class.som3.tx=as.factor(class.som3.tx)
table.som3.tx=dat %>% group_by(class.som3.tx) %>% tally()
table.som3.tx
plot(som.out3.tx, type = 'codes', bgcol = rainbow(3)[class.cuts3.tx])
add.cluster.boundaries(som.out3.tx, class.cuts3.tx)
plot(som.out3.tx, type = 'changes')
mean.total.table3.tx=aggregate(x=dat$qTot,
by=list(som.out3.tx[["unit.classif"]]),
FUN=mean)
median.total.table3.tx=aggregate(x=dat$qTot,
by=list(som.out3.tx[["unit.classif"]]),
FUN=median)
First Header  | Second Header
------------- | -------------
Content Cell  | Content Cell
Content Cell  | Content Cell
