\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1.0in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{\vspace{1.5in} \textbf{\underline{Comparing Accuracy}}}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \subtitle{\vspace{0.35in} \textbf{Traditional vs Probabilistic Scoring}\\
\textbf{of PHQ9 Data}\\
\vspace{0.5in} \underline{Consultation Summary Report }\\
\vspace{0.5in} \underline{Client:} \hspace{10pt} Alan Malik, PhD -
Patient Tools, inc.\\
\vspace{0.25in} \underline{Consultant:} \hspace{10pt} Lee Panter -
University of Colorado - Denver}
  \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{array}
\usepackage{epsfig}
\usepackage{color}
\usepackage{multirow}
\usepackage{amsrefs}
\usepackage{placeins}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage{comment}
\usepackage{multirow}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage[right]{lineno}

\doublespacing
\setlength\parindent{15pt}
\setlength{\parskip}{3mm plus 1mm minus 1mm}

\lstset{frame=tb,
language=R,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
numbers=none,
keywordstyle=\color{blue},
numberstyle=\tiny\color{gray},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=true,
breakatwhitespace=true,
tabsize=3
}

\begin{document}
\maketitle

\thispagestyle{empty}

\newpage

\thispagestyle{empty}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\begin{singlespace}
\tableofcontents
\end{singlespace}

\newpage

\pagenumbering{arabic}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{background}{%
\subsection{Background}\label{background}}

The Patient Health Questionnaire-Nine (PHQ9) is a nine-question module
used for screening, monitoring, and grading depressive symptoms related
to criteria outlined by the Diagnostic and Statistical Manual of Mental
Health Disorders (DSM-IV) {[}1{]}. The PHQ9 can be administered by
medical staff, or it can be self-administered either electronically or
in paper format. Responses to the nine questions correspond to a
numerical value and a written description of frequencies connected with
activities, feelings, and thoughts related to depression symptoms. The
PHQ9 was originally developed by Dr.~Robert J. Spitzer, Dr.~Janet B.W.
Williams, Dr.~Kurt Kroenke in 1999 with a grant from Pfizer {[}2{]}.

Responses to the PHQ9 are classified into a discrete set of groups which
can be ordered according to the severity of symptoms observed within the
group. This analysis considers classification of PHQ9 data into three
categories with the depression risk assignment values: \(C_{1}\) - ``Not
clinically depressed'', \(C_{2}\) - ``Sub-threshold depression'', and
\(C_{3}\) - ``Major depression''.

Classification of PHQ9 responses is traditionally performed using the
total sum of numerical scores assigned to the answers provided.
Traditional classification asserts that the sum of the provided answers
can be used to classify an observation in conjunction with decision
threshold values. This method has been shown to achieve 88\% accuracy in
previous investigations {[}3{]}, but it assumes that each question is
contributing equally to the outcome. Traditional classification also
requires test takers to answer all nine questions to obtain comparable
classification sum-values. Practitioners suffer from false-positive and
false-negative results that cause patient concern and healthcare system
burdens, ultimately leading to a significant decrease in the effective
implementation of the PHQ9 in the clinical environments for which it was
designed to function {[}4{]}.

An alternative method of classifying PHQ9 observations is Probabilistic
classification. Probabilistic classification improves upon traditional
methods by using a pre-trained algorithm to iteratively re-weight
probabilities representative of being classified within a given
depression risk category until a sufficient confidence threshold is met.
The probabilistic classification algorithm allows for early-stopping
when taking the PHQ9 (certain answer subsets may contain enough
information to probabilistically classify without needing all nine
answers provided). Probabilistic classification is also not reliant on
integer-length distances and similarity measures that limit the
resolution of the traditional classification criteria. The probabilistic
classification algorithm also has the possibilities of gaining accuracy
with training sample size and relating outcomes to actionable
information and treatment plans.

This analysis seeks to compare the accuracy of traditional and
probabilistic classification on PHQ9 data. It also seeks to investigate
the relationship between probabilistic classification accuracy and
training sample size, and determine how this effect interacts with the
accuracy of traditional classification.

\hypertarget{data}{%
\subsection{Data}\label{data}}

The PHQ9 data that is used to conduct this analysis originated from a
Federally Qualified Health Research Center in Montana, United States of
America {[}4{]}. The data was collected over a six month period of time,
employing electronic-tablet administration of the module.

The data sample consists of 2495 observations on 286 variables and is
observational with respect to measurements made on PHQ9 outcomes. In
addition to PHQ9 answer variables, the data also contains demographic
information (age and gender), record-keeping variables (time, date,
record numbers,\ldots{}etc), and the results of another psychological
evaluation that is not considered in this analysis. Observations of PHQ9
variables are integer-valued (0,1,2, and 3), corresponding to the
numerical association of the response provided in the module. Nine
questions constitute a full answer set, and all nine questions had the
same possible answers (0,1,2, and 3) for each question. It is assumed
that questions are provided, and answered sequentially, i.e.~question
ordering was followed. The data has been de-identified, and contains no
missing observations.

The data contains no representation of the response. In other words, the
data contains real responses to PHQ9 tests, and these responses can be
traditionally or probabilistically classified into one of the three
depression risk categories based upon these values; however, the data
does not include any variable that can be used to determine which method
is closer to the underlying truth. The data is self-reported, responses
can be misrepresented or misinterpreted by the participant.

\hypertarget{classification-methods}{%
\subsection{Classification Methods}\label{classification-methods}}

A general framework for the traditional and probabilistic classification
methods is provided. Suppose that the index \(i=1, 2, \ldots, N=2495\)
represents a particular observation from the PHQ9 data, and let
\(q=1, \ 2, \ldots, 9\) represent a specific question within each
response sequence. Each response, to each question in an observation
sequence will be denoted \(A_{iq}=a_{iq}\), where the random value
\(A_{iq}\) is associated with the outcome \(a_{iq}\). References to an
arbitrary (fixed) observation value \(I=i^{*}\) will utilize the
shortened notation: \(A_{q}=A_{i^{*}q}=k\) where
\(k\in \{ 0,\ 1, \ 2, \ 3 \}\).

\hypertarget{traditional-classification}{%
\subsubsection{Traditional
Classification}\label{traditional-classification}}

The traditional classification of observation \(i\) will be denoted:
\[\mathbb{T}_{i}^{c} \in \mathbf{T}_{i}^{C}=\left(\mathbb{T}_{i}^{1}, \ \mathbb{T}_{i}^{2}, \ \mathbb{T}_{i}^{3} \right)\]
The traditional classification of an observation is determined using the
sum of the answer set provided: \[S_{i} = \sum_{q=1}^{9} a_{iq}\]
Traditional depression classification outcomes are distinguished by
threshold values
\newline \(\alpha_{1}, \ \alpha_{2} \ \in \left [0, \ 27 \right ]\) with
\(\alpha_{1} \leq \alpha_{2}\) and outcome sets can then be defined as
\(\mathbb{T}^{c}\) for \(c=1,2,3\):

\begin{center}
$\mathbb{T}^{1}=\left \{i \ \Big | \ S_{i} < \alpha_{1}  \right \}$ \hspace{5pt} $\mathbb{T}^{2}=\left \{i \ \Big | \ \alpha_{1} \leq S_{i} < \alpha_{2}  \right \}$ \hspace{5pt} $\mathbb{T}^{3}=\left \{i \ \Big | \ \alpha_{2} \leq S_{i}  \right \}$
\end{center}

\hypertarget{probabilistic-classification}{%
\subsubsection{Probabilistic
Classification}\label{probabilistic-classification}}

The probabilistic classification of observation \(i\) will be denoted:
\[\mathbb{P}_{i}^{c} \in \mathbf{P}_{i}^{C}=(\mathbb{P}_{i}^{1}, \ \mathbb{P}_{i}^{2}, \ \mathbb{P}_{i}^{3})\]
Given a set of training observations \(\mathbf{\Omega}_{Train}\) of
length \(N_{train}=|\mathbf{\Omega}_{Train}|\), it is possible to
characterize the probability of certain events within
\(\mathbf{\Omega}_{Train}\).

The probability that a response value of \(k\) is provided to question
\(q\) is:
\[P\left(A_{q}=k \right) = \frac{1}{N_{train}} \sum_{i=1}^{N_{train}} \mathbb{I}\left(A_{iq}=k \right)\]
where \[\mathbb{I}\left(A_{iq} = k \right)=
      \begin{cases}
        0 &\mbox{if} \quad  a_{iq} \neq k \\
        1 &\mbox{if} \quad  a_{iq}=k \\  
      \end{cases}
      \] Similarly, the probability that an observation is assigned (via
traditional classification) into into traditional class \(j\) where
\(j \in \left \{ 1, \ 2, \ 3 \right \}\) is:
\[P\left( \mathbb{T}^{C}=j \right) = \frac{1}{N_{Train}} \sum_{i=1}^{N_{train}} \mathbb{I}\left(\mathbb{T}_{i}^{C}=j \right)\]
where \[\mathbb{I}\left(\mathbb{T}_{i}^{C}=j \right)
      \begin{cases}
        0 &\mbox{if} \quad  j \neq c \\
        1 &\mbox{if} \quad  j = c \\  
      \end{cases}
      \] Lastly, the probability that an observation's response value is
\(k \in \left \{ 0, 1, 2, 3 \right \}\) for question \(Q=q\), and that
same observation also has been assigned into traditional classification
class \(\mathbb{T}^{C}=j\) is given by:
\[P\left(A_{q} = k \bigcap \mathbb{T}^{C}=j   \right) =  \frac{1}{|\mathbb{T}_{Train}^{j}|} \sum_{i \in \mathbb{T}_{Train}^{j}} \mathbb{I} \left(A_{iq}=k \right) \]
where we are defining the set:
\[\mathbb{T}_{Train}^{j}= \left \{  i \in \mathbf{\Omega}_{Train} \ \Big | \ i \in \mathbb{T}^{j} \right \} \quad \text{for} \quad j=1,2,3\]
and where the value \(|\mathbb{T}_{Train}^{j}|\) represents the number
of elements in \(\mathbb{T}_{Train}^{j}\)

These probabilities allow for the definition of a weight parameter
corresponding to each \(4\times9\times3 =108\) distinct combination of
question number \(Q=q \in \left \{ 1, \ 2, \ldots, 9 \right \}\),
response value \(A_{q} = k \in \left \{ 0, 1, 2, 3 \right \}\), and
traditional classification:
\(\mathbb{T}^{C} = j \in \left \{ 1, \ 2 , \ 3 \right \}\)
\begin{align*}
  \mathbf{W}_{A_{q}}^{\mathbb{T}^j} &= \frac{P\left(A_{q} = k \ \Big | \ \mathbb{T}^{C}=j \right) }{P\left( A_{q} = k   \right) } \tag{EQ: 1.3.2-1} \label{EQ: 1.3.2-1} \\[0.5em]
&= \frac{P\left(A_{q} = k \ \bigcap \ \mathbb{T}^{C}=j \right)}{P\left( A_{q} = k   \right)P\left( \mathbb{T}^{C}=j  \right)}\\[0.5em]
&= \frac{\frac{1}{|\mathbb{T}_{Train}^{j}|} \sum_{i \in \mathbb{T}_{Train}^{j}} \mathbb{I} \left(A_{iq}=k \right)}{\left( \frac{1}{N_{train}} \sum_{i=1}^{N_{train}} \mathbb{I}\left(A_{iq}=k \right) \right) \left( \frac{1}{N_{Train}} \sum_{i=1}^{N_{train}} \mathbb{I}\left(\mathbb{T}_{i}^{C}=j \right) \right) }\\[0.5em]
&= \frac{N_{Train}^{2}}{|\mathbb{T}_{Train}^{j}|} \times \frac{\sum_{i \in \mathbb{T}_{Train}^{j}} \mathbb{I} \left(A_{iq}=k \right)}{\left( \sum_{i=1}^{N_{train}} \mathbb{I}\left(A_{iq}=k \right)  \right)  \left(  \sum_{i=1}^{N_{train}} \mathbb{I}\left(\mathbb{T}_{i}^{C}=j \right)  \right) }   
\end{align*} Calculating the values of
\(\mathbf{W}_{A_{q}}^{\mathbb{T}^{j}}\) constitutes the training portion
of the probabilistic classification algorithm which now proceeds to
classifying newly introduced data using an iterative approach. A
pre-specified confidence threshold value \(\gamma \in (0,1)\) determines
the stopping point of the algorithm which proceeds according to the
framework:

Let
\(\mathbf{P}_{(q)}^{C}=\left(\mathbb{P}_{(q)}^{1}, \ \mathbb{P}_{(q)}^{2}, \ \mathbb{P}_{(q)}^{3} \right)\)
represent the \(q^{th}\) iteration of the probabilistic re-weighting
algorithm, for \(q=1, \ldots, 9\) \begin{align*}
\left(\mathbb{P}_{(0)}^{1}, \ \mathbb{P}_{(0)}^{2}, \ \mathbb{P}_{(0)}^{1} \right) &= \left( \frac{1}{3}, \  \frac{1}{3}, \ \frac{1}{3}\right) \\[0.5em]
    \left(\mathbb{P}_{(q)}^{1}, \ \mathbb{P}_{(q)}^{2}, \ \mathbb{P}_{(q)}^{3} \right) &= \frac{\left(\mathbb{P}_{(q-1)}^{1}\mathbf{W}_{A_{q}}^{\mathbb{T}^{1}}, \ \mathbb{P}_{(q-1)}^{2}\mathbf{W}_{A_{q}}^{\mathbb{T}^{2}}, \ \mathbb{P}_{(q-1)}^{3}\mathbf{W}_{A_{q}}^{\mathbb{T}^{3}} \right) }{\sum_{j=1}^{3} \mathbb{P}_{(q-1)}^{j}\mathbf{W}_{A_{q}}^{\mathbb{T}^{j}}} \tag{EQ: 1.3.2-2} \label{EQ: 1.3.2-2}
\end{align*} for \(q=1, 2, \ldots, 9\)\\
by defining
\[\mathbb{P}_{q}^{*}= \underset{j \in \left \{1, 2, 3 \right \}}{max} \ \left \{ \mathbb{P}_{q}^{j} \right \} \quad \text{for} \quad q=1, \ldots, 9\]
and
\[q^{*}=\underset{q \in \left \{1, \ldots, 9 \right \}}{min} \ \left \{q \ \Big | \ \mathbb{P}^{*}_{q} > \gamma \right \}\]
then (provided that \(q^{*}\) exists), the probabilistic scoring
classification is: \begin{align*}
\mathbb{P}_{q^{*}}^{*} &= \underset{j \in \left \{1, 2, 3 \right \}}{max} \ \left \{ \mathbb{P}_{q^{*}}^{j} \right \} \\[0.5em]
&= \underset{j \in \left \{1, 2, 3 \right \}}{max} \ \left \{ \underset{q \in \left \{1, \ldots, 9 \right \}}{min} \ \left \{q \ \Big | \ \mathbb{P}^{*}_{q} > \gamma \right \} \right \}
\end{align*}

\hypertarget{consultation-goals-and-deliverables}{%
\subsection{Consultation Goals and
Deliverables}\label{consultation-goals-and-deliverables}}

The initially specified goals provided by Dr.~Alan Malik were:

\begin{quote}
``First, mathematically prove that probabilistic scoring is more
accurate than conventional scoring''
\end{quote}

\begin{quote}
``Second, mathematically prove that probabilistic scoring derived from a
conventional scored validation dataset is essentially as accurate as
using the original validation dataset and therefore still more accurate
than conventional scoring''
\end{quote}

Over the course of this analysis, the objectives shifted to reflect the
lack of response variable and an inability to simulate a comprehensive
response distribution. Mathematical proof was deprioritized in favor of
seeking a comparison of traditional and probabilistic classification
that allowed for accuracy comparisons and demonstrated that
probabilistic classification accuracy improves with training sample
size. The provided data lacked sufficient information to conduct a
formal evaluation of accuracy within the time allotted, and simulated
response values were used to evaluate relative accuracy measures as a
substitute for realistic accuracy measures.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\newpage

\hypertarget{model-and-methods}{%
\section{Model and Methods}\label{model-and-methods}}

\hypertarget{quantifying-accuracy}{%
\subsection{Quantifying Accuracy}\label{quantifying-accuracy}}

Model classification accuracy is calculated as a function of
predicted/fitted value and response value:
\[\text{Accuracy} = f(\mathbf{y}, \mathbf{\hat{y}}) = \frac{1}{N} \sum_{i=1}^{N}  I\left(y_{i}=\hat{y}_{i} \right)\]
where \(\mathbf{y}=\left(y_{1}, \ldots, y_{N}\right)\) are the response
values,
\(\mathbf{\hat{y}}=\left(\hat{y}_{1},\ldots, \hat{y}_{N} \right)\) are
the predicted/fitted values, and\\
\[
I\left(y_{i}=\hat{y}_{i} \right)=
\begin{cases} 
0 &\mbox{if} \quad  y_{i} \neq \hat{y}_{i} \\ 
1 &\mbox{if} \quad  y_{i} = \hat{y}_{i} \\ 
\end{cases}
\] When response values are not available, accuracy can be approximated
as a function of fitted values (\(\mathbf{\hat{y}}\)) and a
\textit{simulated} response
(\(\tilde{\mathbf{y}}=\left(\tilde{y}_{1}, \ldots, \tilde{y}_{N} \right)\)),
where it is assumed that \(\mathbf{\tilde{y}}\approx\mathbf{y}\) but
\(\mathbf{\tilde{y}} \neq \mathbf{y}\) \[
\tilde{\text{Accuracy}} = f(\mathbf{\tilde{y}}, \mathbf{\hat{y}}) = \frac{1}{N} \sum_{i=1}^{N}  I\left(\tilde{y}_{i}=\hat{y}_{i} \right) \tag{EQ: 2.1-1} \label{EQ: 2.1-1}
\] Simulated accuracy calculations can be performed for traditional and
probabilistic classifications: \begin{align*}
\tilde{Accuracy_{\mathbb{T}}} \leftarrow & \rightarrow \tilde{Accuracy_{\mathbb{P}}} \\[0.5em]
f\left(\mathbf{\tilde{y}}, \mathbf{\hat{y}}_{\mathbb{T}} \right) \leftarrow & \rightarrow f\left(\mathbf{\tilde{y}}, \mathbf{\hat{y}}_{\mathbb{P}} \right) \tag{EQ: 2.1-2} \label{EQ: 2.1-2}
\end{align*} This analysis will focus on estimating various values of
\(\tilde{Accuracy}\) \eqref{EQ: 2.1-2} using multiple methods of
generating values of \(\tilde{\mathbf{y}}\)

\hypertarget{cross-validation-processing}{%
\subsection{Cross Validation
Processing:}\label{cross-validation-processing}}

Estimating \(\tilde{Accuracy}\) using Cross Validation (CV) allows for
the estimation of both probabilistic and traditional classifications
simultaneously, and the ability to establish the relationship between
training sample size and accuracy.

Given a fixed value of \(K \in \left \{ 1, 2, \ldots, N \right \}\) a
partition of the original data is formed from \(K\) equal subsets, each
of which has \(N_{K}\) observations sampled randomly without replacement
from the original data, where:
\[N_{K}=\Big \lfloor \frac{N}{K} \Big \rfloor\] There are \(K\)
different ways in which \(K-1\) data sets can be chosen from \(K\) sets
when order does not matter: \begin{align*}
\binom{k}{k-1} &= \frac{k!}{(k-1)!(k-(k-1))!}\\[0.5em]
&= \frac{k!}{(k-1)!}\\[0.5em]
&= k
\end{align*} for each of these \(K\) combinations a train-test
combination is created by combining \(K-1\) selected sets to form the
training set, and the remaining set to form the test set. Let the
\(j^{th}\) train-test pairing in the partition of the full data into
\(K\) sets be represented by:
\[\left(TR, TE \right)^{K}_{j}=\left(TR_{j}^{K}, TE_{j}^{K} \right) \]
for \(j=1, \ldots, K\).

Using the definition of \(N_{K}\), and the fact that each training set
is a union of \(\left(K-1 \right)\) sets of length \(N_{K}\), it is
possible to establish a relationship between training set length,
\(|TR_{j}^{k}|\), and the value of \(K\) from which the original
train-test partitions were formed:\\
\[|TR_{j}^{k}|=(K-1)N_{K}=(K-1)\Big \lfloor \frac{N}{K} \Big \rfloor\] a
similar relationship may be defined for the length of the test set:
\[|TE_{j}^{k}|=N_{K}=\Big \lfloor \frac{N}{K} \Big \rfloor\] note that:
\[\Big \lfloor \frac{N}{K} \Big \rfloor \leq \frac{N}{K}\] implies
\begin{align*}
|TR_{j}^{K}| + |TE_{j}^{K}| &= (k-1)\Big \lfloor \frac{N}{K} \Big \rfloor + \Big \lfloor \frac{N}{K} \Big \rfloor \\[0.5em]
&= K \Big \lfloor \frac{N}{K} \Big \rfloor \\[0.5em]
&\leq K \frac{N}{K} = N
\end{align*} meaning that observations will be left out of the sampling
process in the selection of subsets. This is due to the definition of
the subsample sizes chosen using the floor function which is not
injective on real valued domains.

It is a stated objective of this analysis to establish a relationship
between \(|TR_{j}^{k}|\) and \(\tilde{accuracy}\), idealistically
represented as: \[
\tilde{Accuracy} = \tilde{Accuracy}\left( |TE_{j}^{k}|  \right)  \tag{EQ: 2.2-1} \label{EQ: 2.2-1} 
\] If it can be shown that:
\[\mathbf{\hat{y}}=\mathbf{\hat{y}}\left(|TR_{j}^{K}| \right) \] then
\begin{align*}
\tilde{Accuracy} &= f\left(\mathbf{\tilde{y}}, \mathbf{\hat{y}} \right) \\[0.5em]
&= f\left(\mathbf{\tilde{y}}, \mathbf{\hat{y}} (|TR_{j}^{K}|)  \right) \\[0.5em]
&\Rightarrow \tilde{Accuracy}= \tilde{Accuracy}\left( |TE_{j}^{k}|  \right)
\end{align*} This formula establishes a connection between the value of
\(K\) and \(\tilde{Accuracy}\). Demonstrating that by calculating the
value of \(\tilde{Accuracy}\) at an appropriately defined variety of
\(K\) values, it is possible to establish the relationship between
\(\tilde{Accuracy}\) and \(|TR_{j}^{k}|\) (i.e.~Training Sample Length).

\eqref{EQ: 2.1-1} can be used to write an expression for the
classification accuracy of each train-test pair within a specific value
of K:
\[\tilde{Accuracy}\left(TR, TE \right)_{j}^{k}= \frac{1}{|TE_{j}^{k}|} \sum_{i=1}^{|TE_{j}^{k}|} I\left(\tilde{y}_{i}=\hat{y}_{i} \right)\]
the average of these values is used to represent the accuracy at a
particular K-value \begin{align*}
  \tilde{Accuracy}\left(TR, TE \right)^{k} &= \tilde{Accuracy}_{m}\left(TR, TE \right)_{\bullet}^{k} \\
  &=\frac{1}{K}\sum_{j=1}^{K} \left \{ \tilde{Accuracy}\left(TR, TE \right)_{j}^{k}\right \} \\
  &=  \frac{1}{K}\sum_{j=1}^{K} \left \{ \frac{1}{|TE_{j}^{k}|} \sum_{i=1}^{|TE_{j}^{k}|} I\left(\tilde{y}_{i}=\hat{y}_{i} \right) \right \}
\end{align*} and the process can be applied to traditional and
probabilistic classification methods: \begin{align*}
  \tilde{Accuracy}_{\mathbb{T}}\left(TR, TE \right)^{k} \leftarrow & \rightarrow \tilde{Accuracy}_{\mathbb{P}}\left(TR, TE \right)^{k} \\[0.5em]
  \frac{1}{K}\sum_{j=1}^{K} \left \{ \tilde{Accuracy}_{\mathbb{T}}\left(TR, TE \right)_{j}^{k}\right \} \leftarrow & \rightarrow \frac{1}{K}\sum_{j=1}^{K} \left \{ \tilde{Accuracy}_{\mathbb{P}}\left(TR, TE \right)_{j}^{k}\right \} \\[0.5em]
  \frac{1}{K}\sum_{j=1}^{K} \left \{ \frac{1}{|TE_{j}^{k}|} \sum_{i=1}^{|TE_{j}^{k}|} I\left(\tilde{y}_{i}=\hat{y}_{i}^{\mathbb{T}} \right) \right \} \leftarrow & \rightarrow  \frac{1}{K}\sum_{j=1}^{K} \left \{ \frac{1}{|TE_{j}^{k}|} \sum_{i=1}^{|TE_{j}^{k}|} I\left(\tilde{y}_{i}=\hat{y}_{i}^{\mathbb{P}} \right) \right \}
\end{align*} where \(\hat{y}_{i}^{\mathbb{P}}\) is a probabilistic
classification fitted value, and \(\hat{y}_{i}^{\mathbb{T}}\) is a
traditional classification fitted value. This estimation is also be
applied to a multitude of different simulated validation outcomes
\begin{align*}
\tilde{Accuracy}_{\mathbb{T}or\mathbb{P}}^{h}\left(TR, TE \right)^{k} &= \frac{1}{K}\sum_{j=1}^{K} \left \{ \tilde{Accuracy}_{\mathbb{T}or\mathbb{P}}^{h}\left(TR, TE \right)_{j}^{k}\right \} \\[0.5em]
&=  \frac{1}{K}\sum_{j=1}^{K} \left \{ \frac{1}{|TE_{j}^{k}|} \sum_{i=1}^{|TE_{j}^{k}|} I\left(\tilde{y}_{i}^{h}=\hat{y}_{i}^{\mathbb{T}or\mathbb{P}} \right) \right \}
\end{align*} where the values of \(h\) will be outlined in
\textbf{Section 2.4}. Since notation is obviously cumbersome, indices
displayed will be limited only to those relevant.

\newpage

\begin{figure}
\begin{center}
\includegraphics[width=0.85\textwidth]{KvsTrainlen.jpeg}
\caption{Training and test set observation counts vs K}
\end{center}
\end{figure}

The possible training set observation counts using the CV method
outlined above on the data provided range between 1247 and 2492
observations. The figure also shows that for K-values greater than 1248,
test set sizes are confined to one. Training set sizes were selected for
analysis by sampling 50 different values of K from k=5 to k=2490. It was
assumed that subsets taken from the data which were sampled randomly
without replacement are at least partially representative of the
population as a whole.

\hypertarget{optimal-convergence-criterion}{%
\subsection{Optimal Convergence
Criterion}\label{optimal-convergence-criterion}}

The probabilistic classification process requires the specification of a
confidence level \(\gamma \in (0,1)\) that determines the threshold at
which the algorithm terminates. In this analysis, a confidence level of
\(\gamma=0.75\) is used. This value optimizes probabilistic
classification accuracy of the first simulated validation outcome (see
\textbf{Section 2.4.1}) when the probabilistic weights are trained on
the full data set.

\hypertarget{simulated-validation-outcomes}{%
\subsection{Simulated Validation
Outcomes}\label{simulated-validation-outcomes}}

\hypertarget{inferential-simulations}{%
\subsubsection{Inferential Simulations}\label{inferential-simulations}}

The first simulated outcome is generated using a series of assumptions
that collectively determine a method for creating a response
distribution uniquely functional on the PHQ9 data inputs and random
sampling.

The three theoretical assumptions made to generate the outcomes are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Depression classifications are a hierarchy, so outcomes in group 1 are
  generally lower than those in group 2, and outcomes in group 2 are
  generally lower than those in group 3
\item
  Within each class there is a spectrum of conditions, and transitions
  between classes exhibit continuous properties. This means that higher
  group 2 outcomes are closer in magnitude to a lower group 3 outcomes
  compared to lower group 2 outcomes.
\item
  The 12 percent misclassification specified in \textbf{Section 1.1} is
  attributable to the traditional classification algorithm
  mis-classifying observations into a proximal class. This means that
  the traditional classification algorithm misclassifies according to:
  group 1 for group 2, group 3 for group 2, and group 2 for either group
  1 or 2; however, it never mistakes group 1 for group 3 or group 3 for
  group 1.
\end{enumerate}

The deterministic traditional classification algorithm outcome is
altered using the three assumptions by implementing quantity variations
related to:

\begin{itemize}
\tightlist
\item
  \(\mathbb{S}_{T}=\left \{ i\ \Big | \ S_{i}=T \right \}\) for
  \(i=1,\ldots, N\) and \(T=0, 1,\ldots, 27\) where
  \(S_{i} = \sum_{q=1}^{9} a_{q}\), and the number of observations with
  the same traditional sum-score (\(S_{i}\)) is the value:
  \(|\mathbb{S}_{T}|\). Traditional sum-scores with higher observation
  counts will have more variation induced.
\item
  Distance from a threshold value. Observations which are closer to a
  threshold value are more likely to be mis-classified, if
  \(\alpha_{1} \leq \alpha_{2} \in \ \left \{ 0,1,\ldots, 27 \right \}\)
  are threshold values then the distance to the nearest threshold value
  of an observation \(i\) that is classified into traditional sum value
  \(S_{i}\) is: \[
  D\left(\alpha_{k}, S_{i}   \right)  = 
  \begin{cases}
  \underset{k=1,2}{min}\left \{ \Big | \alpha_{k}-S_{i}   \Big | \right \} &\mbox{if} \quad  \alpha_{k} \neq S_{i} \\ 
  \frac{1}{2} &\mbox{if} \quad  \alpha_{k} = S_{i} \\
  \end{cases}
  \] The final probability of a traditional classification being altered
  (changed from its traditional classification) in its representation in
  the simulated outcome data set is then:
  \[P\left(\tilde{\mathbb{T}}_{i} \neq \mathbb{T}_{i} \right) \propto \frac{|\mathbb{S}_{T}|}{D\left(\alpha_{k}, S_{T} \right) }\]
  where proportionality constants are chosen for each value of
  \(T=0,1,\ldots, 27\) so that the total difference between Traditional
  Scoring classification and the simulated outcomes is approximately
  12\%
\end{itemize}

Specific values of
\(P\left(\tilde{\mathbb{T}}_{i} \neq \mathbb{T}_{i} \right)\) that were
used for this analysis are displayed in Table 1 below.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
$\mathbb{S}_{T}$ & Orig Class & Sim Class & $D_{\alpha_{1}}$ & $D_{\alpha_{2}}$ & $|\mathbb{S}_{T}^{PR}|$ & \# Flip & Prop Const & \% Total Data \\
\hline
\hline
5 & 1 & 2 & 2 & 5 & 149 & 42 & 0.282 & 0.017 \\
\hline
6 & 1 & 2 & 1 & 4 & 137 & 63 & 0.460 & 0.025 \\
\hline
7 & 2 & 1 & $\frac{1}{2}$ & 3 & 108 & 42 & 0.389 & 0.017 \\
\hline
8 & 2 & 1 & 1 & 2 & 115 & 19 & 0.165 & 0.008 \\
\hline
8 & 2 & 3 & 1 & 2 & 115 & 21 & 0.183 & 0.008 \\
\hline
9 & 2 & 1 & 2 & 1 & 134 & 16 & 0.119 & 0.006 \\
\hline
9 & 2 & 3 & 2 & 1 & 134 & 21 & 0.157 & 0.008 \\
\hline
10 & 3 & 2 & 3 & $\frac{1}{2}$ & 72 & 21 & 0.292 & 0.008 \\
\hline
11 & 3 & 2 & 4 & 1 & 73 & 21 & 0.288 & 0.008 \\
\hline
12 & 3 & 2 & 5 & 2 & 77 & 21 & 0.273 & 0.008 \\
\hline
 &  &  &  &  &  &  & SUM & 0.113 \\
\hline
\end{tabular}
\end{center}
\caption{Inferentially simulated outcome constants}
\end{table}

\newpage

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.85\textwidth]{ClassFlips.jpeg}
\end{center}
\caption[Proportion of Induced Variations]{observational distributions as classified traditionally, with the proportion of induced change imposed}
\end{figure}

Since the simulated validation outcomes generated using this method
depended only on information relevant to the observation from which the
outcome was being simulated, the simulated data values are paired with
their corresponding observations prior to evaluating the CV algorithm.
Once paired, 50 accuracy calculations of probabilistic and traditional
classification are obtained on 50 different values of K using the CV
algorithm previously outlined.

This simulated outcome is implemented in the process to obtain the
optimal convergence confidence level in \textbf{Section 2.3}

\hypertarget{probabilistic-outcome-simulations}{%
\subsubsection{Probabilistic Outcome
Simulations}\label{probabilistic-outcome-simulations}}

The probabilistic classification algorithm is appealing because it
offers multiple advantages over traditional classification. Although not
fully confirmed, intuition indicates that the accuracy of the
probabilistic classification should increase with sample size. Moreover,
the probabilistic classification algorithm considers the information
each question response provides independently, considering responses as
sequences of information instead of a single agglomeration point.

Together, these aspects make the probabilistic algorithm not only
appealing for classification, but also simulation. Similar to the
inferences conducted in \textbf{Section 2.4.1}, the probabilistic
classification algorithm estimates can be used as a simulated responses
to conduct an analysis of accuracy.

The process involves altering the CV algorithm to allow for estimation
of probabilistic outcomes, traditional outcomes, and a distinct
probabilistically simulated response, simultaneously.

As before, the original data is separated into K-CV sets, and combined
into train-test pairings
\[(TR, TE)_{j}^{K} \quad \text{for} \quad j=1, \ldots, K\] For each
training set the corresponding set of probabilistic training weights is
calculated
\[TR_{j}^{K} \ \rightarrow \ \eqref{EQ: 1.3.2-1} \ \rightarrow \  W_{A_{q}}^{\mathbb{T}_{j}}\]
K train-test pairs means there will be K distinct estimates of the
training weights. The average training weight taken across the
K-estimates is used for the classification of new response values:
\[W_{A_{q}}^{\mathbb{T}_{K}}= W_{A_{q}}^{\mathbb{T}^{\bullet}}=\frac{1}{K}\sum_{j=1}^{K} W_{A_{q}}^{\mathbb{T}^{j}}\]
\[\left(W_{A_{q}}^{\mathbb{T}_{K}}, TE_{j}^{K}\right)  \ \rightarrow \  \eqref{EQ: 1.3.2-2} \ \rightarrow \   \tilde{y}_{j}^{K}\]
The simulated outcomes generated using this method depend on information
obtained from a training set used to generate the probabilistic weights
which are then used in the response classifications. Each simulated
outcome is therefore paired with the data that is used to simulate it
inside of the CV algorithm. \begin{align*}
  \tilde{accuracy}_{\mathbb{T}or\mathbb{P}}\left(TR, TE \right)^{k} &= \frac{1}{K}\sum_{j=1}^{K} \left \{\tilde{accuracy}_{\mathbb{T}or\mathbb{P}} \left(TR, TE)_{j}^{k}   \right)   \right \} \\
  &= \frac{1}{K}\sum_{j=1}^{K} \left \{ \frac{1}{|TE_{j}^{k}|} \sum_{i=1}^{|TE_{j}^{k}|} \left \{ I\left(\tilde{y}_{ji}^{K}=\hat{y}_{ji}^{K\left(\mathbb{T}or\mathbb{P} \right) } \right) \right \}   \right \}
\end{align*}

This method makes the assumption that the estimates of average weights
are stable enough to produce reasonably consistent estimates

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.7\textwidth]{VariancePlot.jpg}
\end{center}
\caption[Probabilistic Simulation Weight Variances]{Variance of average weight values for several question number/class ID combinations plotted against training data count length}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.7\textwidth]{MeanPlot.jpg}
\end{center}
\caption[Probabilistic Simulation Weight Averages]{Mean value of the average weight estimates for several question number/class ID combinations plotted against training data count length}
\end{figure}

\newpage

The stability of the average estimate value, as training sample
increases, in combination with a dramatic decrease in variance as
training sample size increases are indications that the average weight
values are likely capable of producing stable outcome estimates

\hypertarget{unsupervised-learning-classifications}{%
\subsubsection{Unsupervised Learning
Classifications}\label{unsupervised-learning-classifications}}

Unsupervised learning methods can be used to discover patterns in data
without the need to specify a response variable or include additional
assumptions. The methods employed in this analysis attempt to derive
trends in the PHQ9 data and infer an outcome by deriving similarities in
the observations using the predictor variables, and associating ``more
similar'' observations with an unknown outcome variable. The
ranked-outcomes \(C_{1}, \ C_{2}, \ C_{3}\) in the PHQ9 data allow for a
hierarchy to be imposed upon the unknown variables based upon the
characteristics of the observations contained within each unknown
outcome. This hierarchy can then be input into the CV algorithm as a
simulated outcome to obtain estimates of traditional and probabilistic
classification accuracies.

\hypertarget{kmeans-clustering}{%
\paragraph{Kmeans Clustering}\label{kmeans-clustering}}

An appropriate number of clusters in the Kmeans algorithm can be
determined with the use of an elbow plot. The first instance of
diminishing returns is two or three clusters. Since the PHQ9 data needs
to be grouped into three clusters this coincides with intuition:

\newpage

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.85\textwidth]{ElbowPlot.jpeg}
\end{center}
\caption[Kmeans Clustering Elbow Plot]{A plot depicting the total within sum of squares vs the number of clusters chosen for the algorithm}
\end{figure}

The Kmeans algorithm seeks to minimize the total sum of squares distance
from the assigned cluster centers. A value of 2000 random
initializations with a maximum iteration per initialization of 1000
steps, and a random seed of ``123'' is utilized for this analysis.

Cluster assignments are formed using the median traditional sum value
within each cluster:

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Cluster Label & Cluster Population & Median Traditional Sum Value \\
\hline
\hline
C1 & 1254 & 2 \\
\hline
C2 & 776 & 9 \\
\hline
C3 & 465 & 19 \\
\hline
\end{tabular}
\end{center}
\caption[Kmeans Cluster assignments]{Kmeans cluster assignments based upon median traditional sum value}
\end{table}

\hypertarget{hierarchical-clustering}{%
\paragraph{Hierarchical Clustering}\label{hierarchical-clustering}}

Two hierarchical clustering variables are created. The first
distinguishes observations based upon Euclidean distance. A circular
dendrogram, with corresponding classifications is displayed in Figure 6a
{[}5{]}. The second hierarchical clustering technique distinguishes
observations based upon Manhattan distance. A circular dendrogram, with
corresponding classifications is displayed in Figure 6b {[}5{]}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.85\textwidth]{CircularDendos.jpg}
\end{center}
\caption[Circular Dendrograms for Hierarchical Clustering]{ \\ Figure 6a (left)-Circular dendrogram with three-category classifications and Euclidean Metric \\ Figure 6b (right)-Circular dendrogram with three-category classifications and Manhattan Metric}
\end{figure}

\newpage

Cluster assignments are formed using the median traditional sum value
within each cluster:

\begin{table}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Cluster &  Pop $L^{2}$ & Med. Sum  $L^{2}$ & Pop. $L^{1}$ & Med. Sum $L^{1}$\\
\hline
\hline
C1 & 1768 & 3 & 1834 & 4\\
\hline
C2 & 246 & 10 & 261 & 13 \\
\hline
C3 & 481 & 18 & 400 & 19 \\
\hline
\end{tabular}
\end{center}
\caption[Hierarchical Cluster assignments]{Hierarchical clustering with Euclidean ($L^{2}$) metric and Manhattan ($L^{1}$) metric assignments based on median traditional sum value}
\end{table}

\hypertarget{self-organizing-maps}{%
\paragraph{Self Organizing Maps}\label{self-organizing-maps}}

Self-organizing maps (SOMs) require a pre-specified structure (generally
a lattice structure) to which observations are clustered based upon
projected proximity {[}6{]}. SOMs are not restricted to linear model
frameworks, and they preserve topological relationships in the mapping
from higher dimensions {[}6{]}. SOMs have useful visualization methods
that are meaningful and can be used for deduction and inference {[}7{]}.

In this analysis SOMs are fit to a 15x15 lattice structure organized in
a hexagonal topology with both \(L^{2}\) and \(L^{1}\) metrics. Data is
standardized prior to completing all SOM clustering procedures. Plots of
each of these mappings are found in Figure 7a (\(L^{2}\) metric) and
Figure 7b (\(L^{1}\) metric) below

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.85\textwidth]{SOMmappings.jpg}
\end{center}
\caption[15x15 Self Organizing Maps with hexagonal topologies]{\\ 7a (left)-Euclidean Metric \\ 7b (right)-Manhattan Metric}
\end{figure}

Cluster assignments are formed using the median traditional sum value
within each cluster:

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Cluster &  Pop $L^{2}$ & Med. Sum  $L^{2}$ & Pop. $L^{1}$ & Med. Sum $L^{1}$\\
\hline
\hline
C1 & 1790 & 3 & 2006 & 4\\
\hline
C2 & 122 & 15 & 155 & 16 \\
\hline
C3 & 583 & 21 & 334 & 21 \\
\hline
\end{tabular}
\end{center}
\caption[Self Organizing Map Cluster assignments]{Self Organizing Map clustering with Euclidean ($L^{2}$) metric and Manhattan ($L^{1}$) metric assignments based upon median traditional sum value}
\end{table}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\newpage

\hypertarget{analysis-and-results}{%
\section{Analysis and Results}\label{analysis-and-results}}

\hypertarget{inferential-outcome-simulation}{%
\subsection{Inferential Outcome
Simulation}\label{inferential-outcome-simulation}}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.85\textwidth]{Sup1Graph.jpg}
\end{center}
\caption[Inferential Outcome Simulation Result Plot]{estimated accuracy values for probabilistic and traditional classifications as calculated using inferentially simulated outcome variables, and calculated across 50 values of training set definitions.  Also plotted are Linear Regression trend lines for each estimation method's accuracy estimates.}
\end{figure}

After simulating an outcome using the Inferential Simulation method
outlined in \textbf{Section (2.4.1)} accuracy values of probabilistic
and tradition classification are calculated and plotted for 50 different
definitions of training sample length. The results of these calculations
indicate that, when measured against inferentially simulated outcomes,
traditional scoring classification is more accurate than probabilistic
scoring for all defined training samples. The results indicate that
probabilistic classification accuracy grows at a faster rate than
traditional classification accuracy; however this result lacks
sufficient evidence to support the claim of a significant finding
(p=0.5581). A regression summary table can be found in Appendix A1.

\newpage

\hypertarget{probabilistic-simulations}{%
\subsection{Probabilistic Simulations}\label{probabilistic-simulations}}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.85\textwidth]{Sup2Graph.jpg}
\end{center}
\caption[Probabilistic Simulations Result Plot]{estimated accuracy values for probabilistic and traditional classifications as calculated using a probabilistically simulated outcome variable, and calculated across 50 values of training set definitions.  Also plotted are Linear Regression trend lines for each estimation method's accuracy estimates.}
\end{figure}

After simulating an outcome using the Probabilistic Simulation method
outlined in \textbf{Section (2.4.2)} accuracy values of probabilistic
and tradition classification are calculated and plotted for 50 different
definitions of training sample length. The results of this calculation
show that, when measured against the probabilistically simulated
outcome, probabilistic classification is more accurate than traditional
classification for all defined training samples. The results indicate
that probabilistic scoring accuracy grows at a faster rate than
traditional scoring accuracy, and that this result is supported by
evidence in the probabilistically simulated data (p=0.00421). A
regression summary table can be found in Appendix A2.

\newpage

\hypertarget{unsupervised-algorithm-outcomes}{%
\subsection{Unsupervised Algorithm
Outcomes}\label{unsupervised-algorithm-outcomes}}

\hypertarget{kmeans-clustering-1}{%
\subsubsection{Kmeans Clustering}\label{kmeans-clustering-1}}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.85\textwidth]{KMeansResultsGraph.jpeg}
\end{center}
\caption[Kmeans Clustering Result Plot]{estimated accuracy values for probabilistic and traditional classifications as calculated using outcomes resulting from a Kmeans clustering process.  The accuracy values are calculated across 50 values of training set definitions.  Also plotted are Linear Regression trend lines for each estimation method's accuracy estimates.}
\end{figure}

A Kmeans unsupervised learning algorithm was used to simulate response
values and then calculate the accuracy of probabilistic and traditional
classification methods for 50 different definitions of training sample
length. The results of this calculation show that, when measured against
responses generated from a Kmeans unsupervised learning algorithm,
probabilistic classification is more accurate than traditional
classification for all training samples defined. The results indicate
that probabilistic scoring accuracy does not continue to grow as
training sample size increases; however, this result lacks sufficient
evidence to support the claim of a significant finding (p=0.874). A
regression summary table can be found in Appendix A3.

\newpage

\hypertarget{hierarchical-clustering-1}{%
\subsubsection{Hierarchical
Clustering}\label{hierarchical-clustering-1}}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.85\textwidth]{HCResultsGraph.jpeg}
\end{center}
\caption[Hierarchical Clustering Result Plot]{estimated accuracy values for probabilistic and traditional classifications as estimated using outcomes resulting from a hierarchical clustering process.  The accuracy values are calculated across 50 values of training set definitions.  Also plotted are Linear Regression trend lines for each estimation method's accuracy estimates.}
\end{figure}

A hierarchical clustering learning algorithm was used to generate
response values and then calculate the accuracy of probabilistic and
traditional classification methods for 50 different definitions of
training sample length. The results of this calculation show that, when
measured against responses generated from a hierarchical clustering
algorithm, traditional classification is more accurate than
probabilistic scoring for all training samples defined. The results
indicate that probabilistic classification accuracy increases with
training sample size; however, this result lacks sufficient evidence to
support the claim of a significant finding (p=0.135). A regression
summary table can be found in Appendix A4.

\hypertarget{self-organizing-maps-1}{%
\subsubsection{Self Organizing Maps}\label{self-organizing-maps-1}}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.85\textwidth]{SomResultsGraph.jpeg}
\end{center}
\caption[Self Organizing Maps Result Plot]{estimated accuracy values for probabilistic and traditional classifications obtained using outcomes generated by two Self Organizing Maps that have a 15x15 lattice structure, a hexagonal topology, and employed either a Euclidean or Manhattan metric.  Also plotted are Linear Regression trend lines for each estimation method's accuracy estimates.}
\end{figure}

Self Organizing Maps were used to simulate response values, which were
used to calculate the accuracy of probabilistic and traditional
classification methods for 50 different definitions of training sample
length. The results of this calculation show that, when measured against
responses generated by a 15x15 hexagonal topology lattice SOM (either
Euclidean or Manhattan metrics), traditional classification is more
accurate than probabilistic classification for all training samples
defined. The results indicate that probabilistic classification accuracy
increases with training sample size; however, in both SOM simulations
(Euclidean and Manhattan metrics) this result lacks sufficient evidence
to support the claim of a significant finding (p=0.1264, \(L^{2}\)) and
(p=0.8843, \(L^{1}\)). A regression summary table can be found in
Appendix A5.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\newpage

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{summary-of-methods}{%
\subsection{Summary of Methods}\label{summary-of-methods}}

This analysis has performed calculations of probabilistic and
traditional classification accuracy using multiple outcomes simulated
from information contained in PHQ9 data that should reasonably represent
a hierarchy of depression classification categories. It is hoped that by
providing a spectrum of theoretically reasonable accuracy measurements,
this analysis can begin to elucidate those values of accuracy that are
likely to represent reality. Six outcome variables were simulated in
three different ways:

\begin{itemize}
\tightlist
\item
  Inferential Simulations are based on theoretical information relating
  to experimentation
\item
  Probabilistic Simulations are also based on theoretical information
  relating to experimentation, but also incorporate empirical evidence
  in the data
\item
  Unsupervised Methods do not account for any underlying experimentation
  practices. Outcomes generated using these methods are based on a
  hierarchy of relations imposed on the results of a clustering
  algorithm.
\end{itemize}

Calculations were evaluated at 50 different definitions of training
sample length, and trend lines were fit to these calculated accuracy
values to estimate the relationship between accuracy and training sample
length.

\hypertarget{summary-of-results}{%
\subsection{Summary of Results}\label{summary-of-results}}

\begin{table}[!h]
\begin{center}
\begin{tabular}{|c||c|c|c|}
\hline
\textbf{Outcome}       &  \textbf{Superior}     &  \textbf{Min Train}   &  \textbf{Max Train }\\
\textbf{Simulation}    &  \textbf{Model}        &  \textbf{Set Size }   &  \textbf{Set Size  }\\  
\textbf{Method}        &  \textbf{Accuracy}     &  \textbf{Equality }   &  \textbf{Equality  }\\  
\hline
\hline
\textbf{Inferential}   & Traditional   &  1.256e4     &  1.381e4    \\
\hline 
\textbf{Probabilistic} & Probabilistic &  NA          &  NA          \\
\hline
\textbf{KMeans}        & Probabilistic &  NA          &  NA          \\
\hline
\textbf{Hierarchical}  & Traditional   &  2.455e4     &  4.135e4     \\
\hline
\textbf{SOM}           & Traditional   &  6.176e3     &  7.106e3     \\
\textbf{Euclidean}     &               &              &              \\
\hline
\textbf{SOM}           & Traditional   &  5.500e3     &  6.122e3     \\
\textbf{Manhattan}     &               &              &              \\
\hline
\end{tabular}
\end{center}
\caption{Analysis Results Summary}
\end{table}

Table 5 above lists the modeling method found to be more accurate with
each corresponding simulated response. In the cases where probabilistic
classification is not the preferred method, Table 5 also lists a range
of estimated training set sizes that, if surpassed, would theoretically
allow probabilistic classification accuracy to exceed traditional
classification accuracy when measured against the corresponding
response.

Training set size ranges in Table 5 are obtained by solving for the
intersection of trend lines in each simulation method. Minimum range
value estimates assume that traditional scoring accuracy trends have
zero accuracy growth, and maximum range value estimates assume the
estimated positive growth rate in the traditional classification
accuracy trend line.

The analysis conducted concludes without a definitive result concerning
the comparison of probabilistic and traditional classification accuracy.
In two of the six simulated outcomes evaluated, probabilistic
classification accuracy is judged to be superior to traditional
classification accuracy. As this answer is not definitive, this analysis
has failed to produce a directly usable comparison of probabilistic and
traditional classification accuracy.

Conversely, this analysis has produced initial confirmational evidence
that probabilistic classification accuracy increases with training
sample size. Excluding only the Kmeans simulated response, the
probabilistic classification accuracy had a higher positive trend than
did traditional scoring classification accuracy. It should be noted that
further research is required in order to provide a complete range of
estimated training sample set sizes over which accuracy can be
calculated.

The Kmeans simulated outcome is expected. Intuition would dictate that
obtaining more observations should not decrease probabilistic scoring
accuracy. However, increasing observational counts will increase
initialization dependencies in the Kmeans algorithm. Therefore, the
decrease in probabilistic scoring classification accuracy is reasonably
explained by a tendency of the outcome variable to ``shift its
preference'' when larger training data sizes are provided.

\hypertarget{client-satisfaction-concerns}{%
\subsection{Client Satisfaction
Concerns}\label{client-satisfaction-concerns}}

As initially specified, this analysis has not completed either of the
client-specified goals. Mathematical proof aside, this analysis has
failed to produce any concrete comparison of probabilistic and
traditional scoring accuracies. The CV analysis has also failed to
produce reliable results and needs to be adapted to allow for more
flexible sampling practices. Additionally, the analysis does not address
or even acknowledge the client's second stated goal.

\hypertarget{next-steps}{%
\subsection{Next Steps}\label{next-steps}}

The approach taken in this analysis is left incomplete, with additional
simulations to run, and more extensive training sample size
relationships to explore; however, pursing these paths is not advisable
as a conclusive result would require an alternative approach entirely.
However, improvements could be easily implemented by independently
sampling Test and Training sets, then replacing observations as sampling
takes place. This new process would allow for an increased number of
Train/Test sample size pairings.

Self Organizing Maps have improved algorithms which allow for the
structure on which they are built to be ``optimized''. This so called
``Growing Self Organizing Map'' (GSOM) would better capture information
in the original data than pre-specified structures, and would be an
appropriate algorithmic improvement.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\newpage

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\hypertarget{appendix-a-regression-summary-tables}{%
\subsection{Appendix A: Regression Summary
Tables}\label{appendix-a-regression-summary-tables}}

\textbf{A1: Inferential Outcome Simulations}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Parameter                      & Estimate  & Std. Error & t. Value  & Pr(>|t|) \\
\hline
\hline
Intercept                      &  8.844e-1 &  2.902e-2  &  3.048e1  & <2e-16   \\                             
\hline
ID = Pscore                    & -1.409e-1 &  4.104e-2  & -3.433e0  & 8.840e-4 \\
\hline
\#Train Obs                    &  1.019e-6 &  1.197e-5  &  8.850e-2 & 9.323e-1 \\
\hline
(ID = Pscore):(\#Train Obs) &  1.020e-5 &  1.692e-5  &  6.030e-1 & 5.481e-1 \\
\hline
\end{tabular}
\end{center}
\caption[A1: Inferential Outcome Simulation Regression Summary Tables]{summary table for linear regression trend lines fit to accuracy estimate values displayed in Figure 8}
\end{table}

\textbf{A2: Probabilistic Simulations}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Parameter                      & Estimate  & Std. Error & t. Value  & Pr(>|t|) \\
\hline
\hline
Intercept                      &  8.864e-1 &  2.503e-2  &  3.541e1  & <2e-16   \\
\hline
ID = Traditional               & -8.453e-2 &  3.540e-2  & -2.388e0  & 1.891e-2 \\
\hline
\#Train Obs                    &  3.026e-5 &  1.032e-5  &  2.932e0  & 4.210e-3 \\
\hline
(ID = Traditional):(\#Train Obs) &  -2.956e-5 &  1.460e-5  &  -2.025e0 & 4.562e-2 \\
\hline
\end{tabular}
\end{center}
\caption[A2: Probabilistic Simulation Regression Summary Tables]{summary table for linear regression trend lines fit to accuracy estimate values displayed in Figure 9}
\end{table}

\textbf{A3: Kmeans Clustering}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Parameter                      & Estimate  & Std. Error & t. Value  & Pr(>|t|) \\
\hline
\hline
Intercept                      &  5.389e-1 &  2.264-2  &  2.380e1  & <2e-16   \\
\hline
ID = Traditional               & -4.129e-1 &  3.202e-2  & -1.290e1  & <2e-16 \\
\hline
\#Train Obs                    &  -1.483e-6 &  9.321e-6  &  -1.590e-1  & 8.740e-1 \\
\hline
(ID = Traditional):(\#Train Obs) &  1.082e-5 &  1.318e-5  &  8.210e-1 & 4.130e-1 \\
\hline
\end{tabular}
\end{center}
\caption[A3: Kmeans Clustering Regression Summary Tables]{summary table for linear regression trend lines fit to accuracy estimate values displayed in Figure 10}
\end{table}

\newpage

\textbf{A4: Hierarchical Clustering}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Parameter                      & Estimate  & Std. Error & t. Value  & Pr(>|t|) \\
\hline
\hline
Intercept                      &  3.437e-1 &  2.505e-2  &  1.372e1  & <2e-16   \\
\hline
ID = Traditional               & 3.805e-1 &  3.543e-2  & 1.074e1  & <2e-16 \\
\hline
\#Train Obs                    &  1.548e-5 &  1.031e-5  &  1.501e0  & 1.350e-1 \\
\hline
(ID = Traditional):(\#Train Obs) &  -9.187e-6 &  1.459e-5  &  -6.300e-1 & 5.300e-1 \\
\hline
\end{tabular}
\end{center}
\caption[A4: Hierarchical Clustering Regression Summary Tables]{summary table for linear regression trend lines fit to accuracy estimate values displayed in Figure 11}
\end{table}

\textbf{A5: Self Organizing Maps}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Parameter     & Estimate  & Std. Error & t. Value  & Pr(>|t|) \\
\hline
\hline
Intercept    &  6.227e-1 &  2.814e-2  &  2.213e1  & <2e-16   \\
\hline
ID = $\left(\text{Pscore}, \ L^{1}  \right)$               & -9.234e-2 &  3.979e-2  & -2.321e0  & 2.136e-2 \\
\hline
ID = $\left(\text{Trad}, \ L^{2}   \right)$               & 1.102e-1 &  3.979e-2  & 2.769e0  & 6.170e-3 \\
\hline
ID = $\left(\text{Trad}, \ L^{1}   \right)$               & 2.841e-2 &  3.979e-2  & 7.140e-1  & 4.762e-1 \\
\hline
\#Train Obs  &  1.781e-5 &  1.60e-5  &  1.535e0  & 1.264e-1 \\
\hline
ID = $\left(\text{Pscore}, \ L^{1}  \right)$:(\#Train Obs) &  2.390e-6 &  1.640e-5  &  1.460e-1 & 8.843e-1 \\
\hline
ID = $\left(\text{Trad}, \ L^{2}   \right)$:(\#Train Obs) &  -1.557e-5 &  1.640e-5  &  -9.490e-1 & 3.437e-1 \\
\hline
ID = $\left(\text{Trad}, \ L^{1}   \right)$:(\#Train Obs) &  -1.593e-5 &  1.640e-5  &  -9.710e0 & 3.329e-1 \\
\hline
\end{tabular}
\end{center}
\caption[A5: Self Organizing Maps Regression Summary Tables]{summary table for linear regression trend lines fit to accuracy estimate values displayed in Figure 12}
\end{table}

\newpage

\hypertarget{list-of-figures-and-tables}{%
\subsection{List of Figures and
Tables}\label{list-of-figures-and-tables}}

\thispagestyle{empty}

\begin{singlespace}
\listoffigures
\listoftables
\end{singlespace}

\newpage

\hypertarget{data-and-code-availability}{%
\subsection{Data and Code
Availability}\label{data-and-code-availability}}

\thispagestyle{empty}

Access to code, with instructions, and all data used for the analysis
completed here is available for download at:

\texttt{https://github.com/leepanter/PScoreVSTscore}

\texttt{git@github.com:leepanter/PScoreVSTscore.git}

\hypertarget{references}{%
\subsection{References}\label{references}}

\bibliography{Bib_AccRefSheet}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-kroenke2002phq}{}%
1. Kroenke K, Spitzer RL (2002) The phq-9: A new depression diagnostic
and severity measure. \emph{Psychiatric annals} 32: 509--515.

\leavevmode\hypertarget{ref-kroenke2010instruction}{}%
2. Kroenke K, Spitzer R (2010) Instruction manual: Instructions for
patient health questionnaire (phq) and gad-7 measures.

\leavevmode\hypertarget{ref-kroenke16spitzer}{}%
3. Kroenke K Spitzer, rl \& williams, jb (2001). The phq-9.
\emph{Journal of General Internal Medicine} 16: 606--613.

\leavevmode\hypertarget{ref-MalikMoreEf}{}%
4. Malik A More effective and cost effective use of the phq-9.

\leavevmode\hypertarget{ref-STHDA_2020}{}%
5. (2020) \emph{Sthdacom}.

\leavevmode\hypertarget{ref-Belavkin}{}%
6. Belavkin R Lecture 13: Self-organising maps.

\leavevmode\hypertarget{ref-Tanner_2020}{}%
7. Tanner D (2020) Introduction to self organizing maps in r - the
kohonen package and nba player statistics. \emph{Githubio}.


\end{document}
